{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nattaran/HealthTequity-LLM/blob/main/HealthTequity_VoicePipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f46ec9ac",
      "metadata": {
        "id": "f46ec9ac"
      },
      "source": [
        "\n",
        "# HealthTequity Voice Pipeline\n",
        "\n",
        "## Introduction <a id=\"introduction\"></a>\n",
        "This notebook presents a modular voice pipeline for bilingual (Spanish–English) processing and evaluation. The system performs the following sequence:\n",
        "1) Automatic Speech Recognition (ASR) on Spanish audio inputs (Whisper base).  \n",
        "2) Machine translation (Spanish → English) for downstream analysis.  \n",
        "3) Question answering over a tabular blood-pressure dataset using an LLM (GPT-40).  \n",
        "4) Back-translation to Spanish followed by text-to-speech (TTS).  \n",
        "5) Re-transcription of the generated Spanish audio using ASR and evaluation (WER, CER, SER).\n",
        "\n",
        "All steps are clearly separated, reproducible, and designed to be executed independently.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ql1V4qibG7Ws"
      },
      "id": "Ql1V4qibG7Ws",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b9ecd8b4",
      "metadata": {
        "id": "b9ecd8b4"
      },
      "source": [
        "\n",
        "## Table of Contents\n",
        "1. [Introduction](#introduction)  \n",
        "2. [Environment Setup](#setup)  \n",
        "3. [Folder Configuration](#paths)  \n",
        "4. [OpenAI Key Initialization](#openai)  \n",
        "5. [Step 1 – ASR and Translation](#asr-translation)  \n",
        "6. [Step 2 – ASR Evaluation](#asr-eval)  \n",
        "7. [Step 3 – LLM Question Answering](#qa)  \n",
        "8. [Step 4 – Translation and TTS](#tts)  \n",
        "9. [Step 5 – Whisper Evaluation of TTS](#tts-eval)  \n",
        "10. [Step 6 – Results Summary](#summary)  \n",
        "11. [Appendix](#appendix)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23d83703",
      "metadata": {
        "id": "23d83703"
      },
      "source": [
        "\n",
        "## Environment Setup <a id=\"setup\"></a>\n",
        "Install dependencies from the provided `requirements.txt`. Run this cell once per runtime.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a53aba9f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a53aba9f",
        "outputId": "cf9f43e2-1c24-4c2f-bb4b-5bcabef0422f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "\n",
        "# Install project requirements (run once per session)\n",
        "# If you prefer to pin versions, ensure requirements.txt includes exact versions.\n",
        "!pip install -r requirements.txt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2af3db58",
      "metadata": {
        "id": "2af3db58"
      },
      "source": [
        "\n",
        "## Folder Configuration <a id=\"paths\"></a>\n",
        "Centralized path configuration for data and results. This version assumes a Drive-based working directory that persists across sessions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5ec22c8",
      "metadata": {
        "id": "b5ec22c8"
      },
      "outputs": [],
      "source": [
        "\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "# Project root in Google Drive (adjust if needed)\n",
        "PROJECT_ROOT = Path(\"/content/drive/MyDrive/HealthTequity-LLM\")\n",
        "\n",
        "# Data and results subfolders\n",
        "DATA_DIR     = PROJECT_ROOT / \"data\"\n",
        "CSV_DIR      = DATA_DIR / \"synthetic_csv\"\n",
        "AUDIO_DIR    = DATA_DIR / \"Spanish_audio\"\n",
        "RESULTS_DIR  = PROJECT_ROOT / \"results\"\n",
        "LLM_OUT      = RESULTS_DIR / \"llm_outputs\"\n",
        "EVAL_DIR     = RESULTS_DIR / \"evaluation_metrics\"\n",
        "TTS_DIR      = RESULTS_DIR / \"tts_audio\"\n",
        "\n",
        "# Create outputs if missing (idempotent)\n",
        "for p in [RESULTS_DIR, LLM_OUT, EVAL_DIR, TTS_DIR]:\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"Project root:\", PROJECT_ROOT)\n",
        "print(\"Data dir:\", DATA_DIR)\n",
        "print(\"CSV dir:\", CSV_DIR)\n",
        "print(\"Audio dir:\", AUDIO_DIR)\n",
        "print(\"Results dir:\", RESULTS_DIR)\n",
        "print(\"LLM outputs:\", LLM_OUT)\n",
        "print(\"Evaluation dir:\", EVAL_DIR)\n",
        "print(\"TTS dir:\", TTS_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "daf53a53",
      "metadata": {
        "id": "daf53a53"
      },
      "source": [
        "\n",
        "## OpenAI Key Initialization <a id=\"openai\"></a>\n",
        "This cell securely initializes the OpenAI client. If the `OPENAI_API_KEY` environment variable is not present, the cell prompts for a key using a hidden input.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2147149",
      "metadata": {
        "id": "d2147149"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "from getpass import getpass\n",
        "from openai import OpenAI\n",
        "\n",
        "# Attempt to load key from environment. Prompt if missing.\n",
        "if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "    print(\"OpenAI API key not found in environment.\")\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Paste your OpenAI API key (input hidden): \").strip()\n",
        "\n",
        "# Initialize client (will raise if key invalid)\n",
        "client = OpenAI()\n",
        "print(\"OpenAI client initialized.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2ead9ad",
      "metadata": {
        "id": "f2ead9ad"
      },
      "source": [
        "\n",
        "## Step 1 – ASR and Translation <a id=\"asr-translation\"></a>\n",
        "This section performs automatic speech recognition (ASR) on Spanish audio using Whisper (**base** model), followed by English translation via the OpenAI API.\n",
        "\n",
        "**Inputs**: `.wav` files under `AUDIO_DIR`.  \n",
        "**Outputs**: `audio_translations.csv` with columns: `audio_file`, `spanish_transcription`, `english_translation`, `language_detected`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0128eb68",
      "metadata": {
        "id": "0128eb68"
      },
      "outputs": [],
      "source": [
        "\n",
        "import whisper\n",
        "import pandas as pd\n",
        "\n",
        "def transcribe_spanish_audio(model, audio_path: Path):\n",
        "    \"\"\"\n",
        "    Transcribe a single Spanish audio file using Whisper.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : whisper.Whisper\n",
        "        Loaded Whisper model instance.\n",
        "    audio_path : Path\n",
        "        Path to the .wav file.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    text : str\n",
        "        Detected transcription text.\n",
        "    lang : str\n",
        "        Detected language code.\n",
        "    \"\"\"\n",
        "    result = model.transcribe(str(audio_path), language=\"es\", task=\"transcribe\", verbose=False)\n",
        "    return result[\"text\"].strip(), result.get(\"language\", \"unknown\")\n",
        "\n",
        "\n",
        "def translate_spanish_to_english(spanish_text: str) -> str:\n",
        "    \"\"\"\n",
        "    Translate a Spanish transcription to English via OpenAI chat completion.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    spanish_text : str\n",
        "        Input Spanish text to translate.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    english_text : str\n",
        "        Translated English text.\n",
        "    \"\"\"\n",
        "    prompt = (\n",
        "        \"Translate the following Spanish medical transcription into clear, faithful English:\\n\\n\"\n",
        "        + spanish_text\n",
        "    )\n",
        "    result = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        temperature=0,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "    )\n",
        "    return result.choices[0].message.content.strip()\n",
        "\n",
        "\n",
        "def process_and_translate_audio(audio_folder: Path, output_csv: Path, model_size: str = \"base\") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Run Whisper ASR on all .wav files in a folder, translate to English, and save results.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    audio_folder : Path\n",
        "        Directory containing .wav files.\n",
        "    output_csv : Path\n",
        "        Destination CSV for transcriptions and translations.\n",
        "    model_size : str, optional\n",
        "        Whisper model size (default: \"base\").\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        DataFrame of results with columns:\n",
        "        [audio_file, spanish_transcription, english_translation, language_detected].\n",
        "    \"\"\"\n",
        "    model = whisper.load_model(model_size)\n",
        "    audio_files = sorted([f for f in os.listdir(audio_folder) if f.endswith(\".wav\")])\n",
        "\n",
        "    results = []\n",
        "    for fname in audio_files:\n",
        "        audio_path = audio_folder / fname\n",
        "        if not audio_path.exists():\n",
        "            continue\n",
        "        es_text, detected_lang = transcribe_spanish_audio(model, audio_path)\n",
        "        en_text = translate_spanish_to_english(es_text)\n",
        "        results.append({\n",
        "            \"audio_file\": fname,\n",
        "            \"spanish_transcription\": es_text,\n",
        "            \"english_translation\": en_text,\n",
        "            \"language_detected\": detected_lang\n",
        "        })\n",
        "    df = pd.DataFrame(results)\n",
        "    df.to_csv(output_csv, index=False, encoding=\"utf-8-sig\")\n",
        "    print(\"Saved:\", output_csv)\n",
        "    return df\n",
        "\n",
        "# Example (do not auto-run):\n",
        "# trans_csv = LLM_OUT / \"audio_translations.csv\"\n",
        "# _ = process_and_translate_audio(AUDIO_DIR, trans_csv, model_size=\"base\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c531344",
      "metadata": {
        "id": "8c531344"
      },
      "source": [
        "\n",
        "## Step 2 – ASR Evaluation (Input) <a id=\"asr-eval\"></a>\n",
        "This section computes WER and CER for the input ASR stage by aligning Whisper transcriptions with ground-truth text.\n",
        "\n",
        "**Assumptions**  \n",
        "- Ground truth is provided in `CSV_DIR / \"ground_truth.csv\"` with columns: `audio_file`, `ground_truth`.\n",
        "- Transcriptions are in `LLM_OUT / \"audio_translations.csv\"` with columns: `audio_file`, `spanish_transcription`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24d16763",
      "metadata": {
        "id": "24d16763"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "from jiwer import wer, cer\n",
        "\n",
        "def evaluate_asr_performance(ground_truth_csv: Path, trans_csv: Path, save_csv: Path) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Compute WER and CER for input ASR against ground truth.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    ground_truth_csv : Path\n",
        "        CSV file with columns: [audio_file, ground_truth].\n",
        "    trans_csv : Path\n",
        "        CSV file with columns: [audio_file, spanish_transcription].\n",
        "    save_csv : Path\n",
        "        Destination CSV for ASR metrics.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        Evaluation results with per-file WER/CER.\n",
        "    \"\"\"\n",
        "    gt_df = pd.read_csv(ground_truth_csv)\n",
        "    tr_df = pd.read_csv(trans_csv)\n",
        "\n",
        "    # Defensive renaming for common variations.\n",
        "    gt_df = gt_df.rename(columns={\"filename\": \"audio_file\", \"spanish_text\": \"ground_truth\"})\n",
        "    tr_df = tr_df.rename(columns={\"transcription\": \"spanish_transcription\"})\n",
        "\n",
        "    df = pd.merge(tr_df[[\"audio_file\", \"spanish_transcription\"]],\n",
        "                  gt_df[[\"audio_file\", \"ground_truth\"]],\n",
        "                  on=\"audio_file\", how=\"inner\")\n",
        "\n",
        "    df[\"WER\"] = [wer(ref, hyp) for ref, hyp in zip(df[\"ground_truth\"], df[\"spanish_transcription\"])]\n",
        "    df[\"CER\"] = [cer(ref, hyp) for ref, hyp in zip(df[\"ground_truth\"], df[\"spanish_transcription\"])]\n",
        "\n",
        "    df.to_csv(save_csv, index=False)\n",
        "    print(\"Saved:\", save_csv)\n",
        "    return df\n",
        "\n",
        "# Example (do not auto-run):\n",
        "# gt_csv  = CSV_DIR / \"ground_truth.csv\"\n",
        "# trans_csv = LLM_OUT / \"audio_translations.csv\"\n",
        "# asr_csv = EVAL_DIR / \"asr_metrics.csv\"\n",
        "# _ = evaluate_asr_performance(gt_csv, trans_csv, asr_csv)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4477ecb",
      "metadata": {
        "id": "f4477ecb"
      },
      "source": [
        "\n",
        "## Step 3 – LLM Question Answering <a id=\"qa\"></a>\n",
        "This section queries an LLM with English questions derived from the ASR+translation step and provides answers based on a tabular blood-pressure dataset.\n",
        "\n",
        "**Inputs**: A CSV file with synthetic blood-pressure records.  \n",
        "**Outputs**: English answers and associated computed fields.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85f89584",
      "metadata": {
        "id": "85f89584"
      },
      "outputs": [],
      "source": [
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "def ask_gpt(question_en: str, csv_block: str) -> dict:\n",
        "    \"\"\"\n",
        "    Query the LLM with a question and a CSV context block.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    question_en : str\n",
        "        English question derived from Spanish transcription.\n",
        "    csv_block : str\n",
        "        CSV content as a single string for in-context grounding.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        A dictionary with keys:\n",
        "        - \"answer\": str (LLM's English answer)\n",
        "        - \"computed_fields\": dict (optional structured fields)\n",
        "    \"\"\"\n",
        "    system = (\n",
        "        \"You are a careful data analyst. Read the CSV and answer the question. \"\n",
        "        \"Only use the information that can be derived from the table.\"\n",
        "    )\n",
        "    user = (\n",
        "        f\"CSV:\\n{csv_block}\\n\\n\"\n",
        "        f\"Question:\\n{question_en}\\n\\n\"\n",
        "        \"Provide a concise answer. If a calculation is required, do it transparently.\"\n",
        "    )\n",
        "    resp = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        temperature=0,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system},\n",
        "            {\"role\": \"user\", \"content\": user},\n",
        "        ],\n",
        "    )\n",
        "    answer = resp.choices[0].message.content.strip()\n",
        "    return {\"answer\": answer, \"computed_fields\": {}}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b90c1a9b",
      "metadata": {
        "id": "b90c1a9b"
      },
      "source": [
        "\n",
        "## Step 4 – Translation and TTS <a id=\"tts\"></a>\n",
        "This section back-translates the LLM's English answers to Spanish and generates Spanish audio (TTS).\n",
        "\n",
        "Note: A simple gTTS-based fallback is provided (exports `.wav` via pydub). If your project includes a custom TTS, replace the fallback with your implementation and keep the same function signature.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "589d5361",
      "metadata": {
        "id": "589d5361"
      },
      "outputs": [],
      "source": [
        "\n",
        "from pathlib import Path\n",
        "\n",
        "def translate_to_spanish(text_en: str) -> str:\n",
        "    \"\"\"\n",
        "    Translate English text to Spanish using OpenAI chat completion.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    text_en : str\n",
        "        English text to translate.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        Spanish translation.\n",
        "    \"\"\"\n",
        "    prompt = \"Translate the following English medical answer into Spanish:\\n\\n\" + text_en\n",
        "    resp = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        temperature=0,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "    )\n",
        "    return resp.choices[0].message.content.strip()\n",
        "\n",
        "\n",
        "def text_to_speech_spanish(text_es: str, out_wav_path: Path):\n",
        "    \"\"\"\n",
        "    Generate Spanish speech from text.\n",
        "\n",
        "    This fallback uses gTTS + pydub to export a WAV file if a custom TTS is\n",
        "    not available. Replace this function with your project-specific TTS if needed.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    text_es : str\n",
        "        Spanish text to synthesize.\n",
        "    out_wav_path : Path\n",
        "        Destination path for the WAV file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from gtts import gTTS\n",
        "        from pydub import AudioSegment\n",
        "        tmp_mp3 = out_wav_path.with_suffix(\".mp3\")\n",
        "        gTTS(text_es, lang=\"es\").save(tmp_mp3)\n",
        "        # Convert to WAV\n",
        "        audio = AudioSegment.from_file(tmp_mp3, format=\"mp3\")\n",
        "        audio.export(out_wav_path, format=\"wav\")\n",
        "        os.remove(tmp_mp3)\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"TTS fallback failed: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af0262a4",
      "metadata": {
        "id": "af0262a4"
      },
      "source": [
        "\n",
        "## Step 5 – Whisper Evaluation of TTS <a id=\"tts-eval\"></a>\n",
        "This section re-transcribes the generated Spanish audio responses using Whisper (base) and evaluates intelligibility against the ground-truth Spanish answers using WER, CER, and SER.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f48ecab5",
      "metadata": {
        "id": "f48ecab5"
      },
      "outputs": [],
      "source": [
        "\n",
        "import re, unicodedata\n",
        "import Levenshtein\n",
        "from jiwer import process_words\n",
        "\n",
        "def normalize_text(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Normalize text for fair ASR comparison.\n",
        "    - Lowercase\n",
        "    - Strip accents\n",
        "    - Remove punctuation\n",
        "    - Collapse extra whitespace\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    text = ''.join(c for c in unicodedata.normalize('NFD', text) if unicodedata.category(c) != 'Mn')\n",
        "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "\n",
        "def evaluate_output_asr_whisper(\n",
        "    tts_csv: Path,\n",
        "    output_csv: Path = None,\n",
        "    model_size: str = \"base\"\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Evaluate Spanish TTS audios using Whisper by computing WER, CER, and SER\n",
        "    against ground-truth Spanish answers.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    tts_csv : Path\n",
        "        CSV with columns: [spanish_answer, audio_answer_file].\n",
        "    output_csv : Path, optional\n",
        "        Destination CSV for ASR metrics (defaults to EVAL_DIR / \"output_asr_metrics_whisper.csv\").\n",
        "    model_size : str, optional\n",
        "        Whisper model size (default: \"base\").\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        DataFrame with per-file metrics.\n",
        "    \"\"\"\n",
        "    if output_csv is None:\n",
        "        output_csv = EVAL_DIR / \"output_asr_metrics_whisper.csv\"\n",
        "\n",
        "    if not tts_csv.exists():\n",
        "        raise FileNotFoundError(f\"Missing final results CSV: {tts_csv}\")\n",
        "\n",
        "    model = whisper.load_model(model_size)\n",
        "\n",
        "    df = pd.read_csv(tts_csv)\n",
        "    rows = []\n",
        "    for _, row in df.iterrows():\n",
        "        gt = str(row.get(\"spanish_answer\", \"\")).strip()\n",
        "        audio_file = str(row.get(\"audio_answer_file\", \"\")).strip()\n",
        "        if not gt or not audio_file or not os.path.exists(audio_file):\n",
        "            continue\n",
        "\n",
        "        # Transcribe generated audio and normalize\n",
        "        res = model.transcribe(audio_file, language=\"es\", task=\"transcribe\", verbose=False)\n",
        "        hyp = res.get(\"text\", \"\").strip()\n",
        "\n",
        "        gt_norm = normalize_text(gt)\n",
        "        hyp_norm = normalize_text(hyp)\n",
        "\n",
        "        measures = process_words(gt_norm, hyp_norm)\n",
        "        wer_score = round(measures.wer, 4)\n",
        "        subs, dels, ins = measures.substitutions, measures.deletions, measures.insertions\n",
        "        cer_score = round(Levenshtein.distance(gt_norm, hyp_norm) / max(len(gt_norm), 1), 4)\n",
        "        ser_score = 0 if gt_norm == hyp_norm else 1\n",
        "\n",
        "        rows.append({\n",
        "            \"audio_file\": os.path.basename(audio_file),\n",
        "            \"ground_truth\": gt,\n",
        "            \"whisper_transcription\": hyp,\n",
        "            \"WER\": wer_score,\n",
        "            \"Substitutions\": subs,\n",
        "            \"Deletions\": dels,\n",
        "            \"Insertions\": ins,\n",
        "            \"CER\": cer_score,\n",
        "            \"SER\": ser_score,\n",
        "        })\n",
        "\n",
        "    out_df = pd.DataFrame(rows)\n",
        "    out_df.to_csv(output_csv, index=False)\n",
        "    print(\"Saved:\", output_csv)\n",
        "    return out_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9911afe5",
      "metadata": {
        "id": "9911afe5"
      },
      "source": [
        "\n",
        "## Orchestration (Optional) <a id=\"orchestration\"></a>\n",
        "The following function orchestrates all steps in sequence. Each step can also be run individually.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "852067e6",
      "metadata": {
        "id": "852067e6"
      },
      "outputs": [],
      "source": [
        "\n",
        "def run_full_pipeline(csv_path: Path, audio_folder: Path, whisper_model_size: str = \"base\"):\n",
        "    \"\"\"\n",
        "    Execute the full pipeline, from input ASR/translation to TTS evaluation.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    csv_path : Path\n",
        "        Path to the synthetic blood-pressure CSV.\n",
        "    audio_folder : Path\n",
        "        Directory containing input Spanish .wav files.\n",
        "    whisper_model_size : str, optional\n",
        "        Whisper model size, default \"base\".\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        Summary dictionary with output artifact locations.\n",
        "    \"\"\"\n",
        "    # Step 1: ASR + Translation\n",
        "    trans_csv = LLM_OUT / \"audio_translations.csv\"\n",
        "    _ = process_and_translate_audio(audio_folder, trans_csv, model_size=whisper_model_size)\n",
        "\n",
        "    # Step 2: Evaluate input ASR\n",
        "    gt_csv = CSV_DIR / \"ground_truth.csv\"\n",
        "    asr_csv = EVAL_DIR / \"asr_metrics.csv\"\n",
        "    _ = evaluate_asr_performance(gt_csv, trans_csv, asr_csv)\n",
        "\n",
        "    # Step 3: Load tabular data for LLM grounding\n",
        "    df_bp = pd.read_csv(csv_path)\n",
        "    csv_block = df_bp.to_csv(index=False)\n",
        "\n",
        "    # Step 4: Q&A + Spanish TTS\n",
        "    results = []\n",
        "    tr_df = pd.read_csv(trans_csv)\n",
        "    for i, row in tr_df.iterrows():\n",
        "        q_en = row[\"english_translation\"]\n",
        "        ans = ask_gpt(q_en, csv_block)\n",
        "        ans_en = ans.get(\"answer\", \"\").strip()\n",
        "        ans_es = translate_to_spanish(ans_en)\n",
        "\n",
        "        out_wav = TTS_DIR / f\"answer_{i+1}_es.wav\"\n",
        "        text_to_speech_spanish(ans_es, out_wav)\n",
        "\n",
        "        results.append({\n",
        "            \"question_number\": i + 1,\n",
        "            \"audio_file_in\": row[\"audio_file\"],\n",
        "            \"spanish_question\": row[\"spanish_transcription\"],\n",
        "            \"english_question\": q_en,\n",
        "            \"english_answer\": ans_en,\n",
        "            \"spanish_answer\": ans_es,\n",
        "            \"audio_answer_file\": str(out_wav),\n",
        "            \"computed_fields\": json.dumps(ans.get(\"computed_fields\", {}))\n",
        "        })\n",
        "\n",
        "    final_csv = LLM_OUT / \"final_pipeline_results.csv\"\n",
        "    pd.DataFrame(results).to_csv(final_csv, index=False)\n",
        "    print(\"Saved:\", final_csv)\n",
        "\n",
        "    # Step 5: Evaluate TTS intelligibility\n",
        "    output_asr_csv = EVAL_DIR / \"output_asr_metrics_whisper.csv\"\n",
        "    _ = evaluate_output_asr_whisper(final_csv, output_csv=output_asr_csv, model_size=whisper_model_size)\n",
        "\n",
        "    return {\n",
        "        \"transcriptions_csv\": str(trans_csv),\n",
        "        \"input_asr_metrics_csv\": str(asr_csv),\n",
        "        \"final_pipeline_csv\": str(final_csv),\n",
        "        \"output_asr_metrics_csv\": str(output_asr_csv),\n",
        "    }\n",
        "\n",
        "# Example (do not auto-run):\n",
        "# bp_csv = CSV_DIR / \"synthetic_bp_one_person.csv\"\n",
        "# _ = run_full_pipeline(bp_csv, AUDIO_DIR, whisper_model_size=\"base\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b1fc405",
      "metadata": {
        "id": "5b1fc405"
      },
      "source": [
        "\n",
        "## Step 6 – Results Summary <a id=\"summary\"></a>\n",
        "This section summarizes average WER, CER, and SER across input ASR and TTS evaluation outputs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23c1ed41",
      "metadata": {
        "id": "23c1ed41"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "def summarize_results(input_asr_csv: Path, output_asr_csv: Path):\n",
        "    \"\"\"\n",
        "    Print dataset-level average metrics for input ASR and TTS ASR evaluation.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_asr_csv : Path\n",
        "        CSV with per-file input ASR metrics.\n",
        "    output_asr_csv : Path\n",
        "        CSV with per-file TTS ASR metrics.\n",
        "    \"\"\"\n",
        "    print(\"Input ASR metrics (WER, CER):\")\n",
        "    if Path(input_asr_csv).exists():\n",
        "        d1 = pd.read_csv(input_asr_csv)\n",
        "        print(d1[[\"WER\", \"CER\"]].mean(numeric_only=True).to_frame(\"Average\"))\n",
        "    else:\n",
        "        print(\"Missing:\", input_asr_csv)\n",
        "\n",
        "    print(\"\\nTTS ASR metrics (WER, CER, SER):\")\n",
        "    if Path(output_asr_csv).exists():\n",
        "        d2 = pd.read_csv(output_asr_csv)\n",
        "        print(d2[[\"WER\", \"CER\", \"SER\"]].mean(numeric_only=True).to_frame(\"Average\"))\n",
        "    else:\n",
        "        print(\"Missing:\", output_asr_csv)\n",
        "\n",
        "# Example (do not auto-run):\n",
        "# summarize_results(EVAL_DIR / \"asr_metrics.csv\", EVAL_DIR / \"output_asr_metrics_whisper.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c776ee9f",
      "metadata": {
        "id": "c776ee9f"
      },
      "source": [
        "\n",
        "## Appendix <a id=\"appendix\"></a>\n",
        "- All paths are centralized under `PROJECT_ROOT` for reproducibility.  \n",
        "- Replace the TTS fallback with a project-specific implementation if available.  \n",
        "- The Whisper model size can be adjusted by changing `whisper_model_size` in the orchestration call.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}