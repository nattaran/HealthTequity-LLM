{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nattaran/HealthTequity-LLM/blob/main/HealthTequity_VoicePipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f46ec9ac",
      "metadata": {
        "id": "f46ec9ac"
      },
      "source": [
        "\n",
        "# HealthTequity Voice Pipeline\n",
        "\n",
        "## Introduction <a id=\"introduction\"></a>\n",
        "This notebook presents a modular voice pipeline for bilingual (Spanish–English) processing and evaluation. The system performs the following sequence:\n",
        "1) Automatic Speech Recognition (ASR) on Spanish audio inputs (Whisper base).  \n",
        "2) Machine translation (Spanish → English) for downstream analysis.  \n",
        "3) Question answering over a tabular blood-pressure dataset using an LLM (GPT-40).  \n",
        "4) Back-translation to Spanish followed by text-to-speech (TTS).  \n",
        "5) Re-transcription of the generated Spanish audio using ASR and evaluation (WER, CER, SER).\n",
        "\n",
        "All steps are clearly separated, reproducible, and designed to be executed independently.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9ecd8b4",
      "metadata": {
        "id": "b9ecd8b4"
      },
      "source": [
        "\n",
        "## Table of Contents\n",
        "1. [Introduction](#introduction)  \n",
        "2. [Environment Setup](#setup)  \n",
        "3. [Folder Configuration](#paths)  \n",
        "4. [OpenAI Key Initialization](#openai)  \n",
        "5. [Step 1 – ASR and Translation](#asr-translation)  \n",
        "6. [Step 2 – ASR Evaluation](#asr-eval)  \n",
        "7. [Step 3 – LLM Question Answering](#qa)  \n",
        "8. [Step 4 – Translation and TTS](#tts)  \n",
        "9. [Step 5 – Whisper Evaluation of TTS](#tts-eval)  \n",
        "10. [Step 6 – Results Summary](#summary)  \n",
        "11. [Appendix](#appendix)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Define the mountpoint\n",
        "mountpoint = '/content/drive'\n",
        "\n",
        "# Check if the mountpoint exists and is not empty, then clear it\n",
        "if os.path.exists(mountpoint) and os.path.isdir(mountpoint) and os.listdir(mountpoint):\n",
        "    print(f\"Clearing mountpoint: {mountpoint}\")\n",
        "    try:\n",
        "        # Use shutil.rmtree to remove the directory and its contents\n",
        "        shutil.rmtree(mountpoint)\n",
        "        # Recreate the directory after removal\n",
        "        os.makedirs(mountpoint)\n",
        "    except Exception as e:\n",
        "        print(f\"Error clearing mountpoint: {e}\")\n",
        "        # If clearing fails, you might want to handle this differently,\n",
        "        # perhaps by raising an error or trying a different mountpoint.\n",
        "        # For now, we'll print an error and proceed, which might lead to\n",
        "        # the same ValueError again, but shows the attempt to clear.\n",
        "        pass\n",
        "\n",
        "\n",
        "drive.mount(mountpoint, force_remount=True)\n",
        "\n",
        "# Clone or update repo inside Drive\n",
        "repo_url = \"https://github.com/nattaran/HealthTequity-LLM.git\"\n",
        "repo_path = \"/content/drive/MyDrive/HealthTequity-LLM\"\n",
        "\n",
        "\n",
        "if not os.path.exists(repo_path):\n",
        "    !git clone {repo_url} {repo_path}\n",
        "else:\n",
        "    %cd {repo_path}\n",
        "    !git fetch origin\n",
        "    !git pull\n",
        "\n",
        "%cd {repo_path}\n",
        "print(\"✅ Environment ready. Working from:\", os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9GGDiUrHnS2",
        "outputId": "be0c19a5-3106-4765-b04d-1f174c0a281b"
      },
      "id": "R9GGDiUrHnS2",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clearing mountpoint: /content/drive\n",
            "Error clearing mountpoint: [Errno 125] Operation canceled: '/content/drive/.Encrypted/MyDrive'\n",
            "Mounted at /content/drive\n",
            "Cloning into '/content/drive/MyDrive/HealthTequity-LLM'...\n",
            "remote: Enumerating objects: 119, done.\u001b[K\n",
            "remote: Counting objects: 100% (119/119), done.\u001b[K\n",
            "remote: Compressing objects: 100% (114/114), done.\u001b[K\n",
            "remote: Total 119 (delta 51), reused 36 (delta 3), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (119/119), 1.99 MiB | 6.71 MiB/s, done.\n",
            "Resolving deltas: 100% (51/51), done.\n",
            "/content/drive/MyDrive/HealthTequity-LLM\n",
            "✅ Environment ready. Working from: /content/drive/MyDrive/HealthTequity-LLM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23d83703",
      "metadata": {
        "id": "23d83703"
      },
      "source": [
        "\n",
        "## Environment Setup <a id=\"setup\"></a>\n",
        "Install dependencies from the provided `requirements.txt`. Run this cell once per runtime.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a53aba9f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a53aba9f",
        "outputId": "f1ff5a32-9b75-4d1d-82c3-7ccf4da2e3c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git (from -r requirements.txt (line 22))\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-z8i0qxbb\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-z8i0qxbb\n",
            "  Resolved https://github.com/openai/whisper.git to commit c0d2f624c09dc18e709e37c2ad90c039a4eb72a2\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (2.0.2)\n",
            "Requirement already satisfied: pandas>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 7)) (2.2.2)\n",
            "Requirement already satisfied: matplotlib>=3.8.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 8)) (3.10.0)\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 11)) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchaudio>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 12)) (2.8.0+cu126)\n",
            "Requirement already satisfied: openai>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 15)) (1.109.1)\n",
            "Requirement already satisfied: tiktoken>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 16)) (0.12.0)\n",
            "Requirement already satisfied: whisper in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 20)) (1.1.10)\n",
            "Requirement already satisfied: ffmpeg-python>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 25)) (0.2.0)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 26)) (0.13.1)\n",
            "Requirement already satisfied: librosa>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 27)) (0.11.0)\n",
            "Requirement already satisfied: pydub>=0.25.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 28)) (0.25.1)\n",
            "Requirement already satisfied: jiwer>=3.0.3 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 31)) (4.0.0)\n",
            "Requirement already satisfied: python-Levenshtein>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 32)) (0.27.1)\n",
            "Requirement already satisfied: gtts>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 35)) (2.5.4)\n",
            "Requirement already satisfied: deep-translator>=1.11.4 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 36)) (1.11.4)\n",
            "Requirement already satisfied: python-dotenv>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 39)) (1.1.1)\n",
            "Requirement already satisfied: tqdm>=4.66.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 40)) (4.67.1)\n",
            "Requirement already satisfied: vosk==0.3.45 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 43)) (0.3.45)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from vosk==0.3.45->-r requirements.txt (line 43)) (2.0.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from vosk==0.3.45->-r requirements.txt (line 43)) (2.32.4)\n",
            "Requirement already satisfied: srt in /usr/local/lib/python3.12/dist-packages (from vosk==0.3.45->-r requirements.txt (line 43)) (3.5.3)\n",
            "Requirement already satisfied: websockets in /usr/local/lib/python3.12/dist-packages (from vosk==0.3.45->-r requirements.txt (line 43)) (15.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.0->-r requirements.txt (line 7)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.0->-r requirements.txt (line 7)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.0->-r requirements.txt (line 7)) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8.0->-r requirements.txt (line 8)) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8.0->-r requirements.txt (line 8)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8.0->-r requirements.txt (line 8)) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8.0->-r requirements.txt (line 8)) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8.0->-r requirements.txt (line 8)) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8.0->-r requirements.txt (line 8)) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8.0->-r requirements.txt (line 8)) (3.2.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (3.4.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.12.0->-r requirements.txt (line 15)) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.12.0->-r requirements.txt (line 15)) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.12.0->-r requirements.txt (line 15)) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.12.0->-r requirements.txt (line 15)) (0.11.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.12.0->-r requirements.txt (line 15)) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai>=1.12.0->-r requirements.txt (line 15)) (1.3.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken>=0.6.0->-r requirements.txt (line 16)) (2024.11.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from whisper->-r requirements.txt (line 20)) (1.17.0)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625->-r requirements.txt (line 22)) (10.8.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625->-r requirements.txt (line 22)) (0.60.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.12/dist-packages (from ffmpeg-python>=0.2.0->-r requirements.txt (line 25)) (1.0.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.10.1->-r requirements.txt (line 27)) (3.0.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.10.1->-r requirements.txt (line 27)) (1.16.2)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.10.1->-r requirements.txt (line 27)) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.10.1->-r requirements.txt (line 27)) (1.5.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.10.1->-r requirements.txt (line 27)) (4.4.2)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.10.1->-r requirements.txt (line 27)) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.10.1->-r requirements.txt (line 27)) (1.0.0)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.10.1->-r requirements.txt (line 27)) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.10.1->-r requirements.txt (line 27)) (1.1.2)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.12/dist-packages (from jiwer>=3.0.3->-r requirements.txt (line 31)) (8.1.8)\n",
            "Requirement already satisfied: rapidfuzz>=3.9.7 in /usr/local/lib/python3.12/dist-packages (from jiwer>=3.0.3->-r requirements.txt (line 31)) (3.14.1)\n",
            "Requirement already satisfied: Levenshtein==0.27.1 in /usr/local/lib/python3.12/dist-packages (from python-Levenshtein>=0.25.0->-r requirements.txt (line 32)) (0.27.1)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.12/dist-packages (from deep-translator>=1.11.4->-r requirements.txt (line 36)) (4.13.5)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai>=1.12.0->-r requirements.txt (line 15)) (3.11)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator>=1.11.4->-r requirements.txt (line 36)) (2.8)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->vosk==0.3.45->-r requirements.txt (line 43)) (2.23)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai>=1.12.0->-r requirements.txt (line 15)) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai>=1.12.0->-r requirements.txt (line 15)) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.12.0->-r requirements.txt (line 15)) (0.16.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->openai-whisper==20250625->-r requirements.txt (line 22)) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa>=0.10.1->-r requirements.txt (line 27)) (4.5.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai>=1.12.0->-r requirements.txt (line 15)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai>=1.12.0->-r requirements.txt (line 15)) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai>=1.12.0->-r requirements.txt (line 15)) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->vosk==0.3.45->-r requirements.txt (line 43)) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->vosk==0.3.45->-r requirements.txt (line 43)) (2.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1.0->librosa>=0.10.1->-r requirements.txt (line 27)) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.1.0->-r requirements.txt (line 11)) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.1.0->-r requirements.txt (line 11)) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Install project requirements (run once per session)\n",
        "# If you prefer to pin versions, ensure requirements.txt includes exact versions.\n",
        "!pip install -r requirements.txt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2af3db58",
      "metadata": {
        "id": "2af3db58"
      },
      "source": [
        "\n",
        "## Folder Configuration <a id=\"paths\"></a>\n",
        "Centralized path configuration for data and results. This version assumes a Drive-based working directory that persists across sessions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "b5ec22c8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5ec22c8",
        "outputId": "3e1b4a83-c613-40c0-b273-5aa32839bd41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Project root: /content/drive/MyDrive/HealthTequity-LLM\n",
            "Data dir: /content/drive/MyDrive/HealthTequity-LLM/data\n",
            "CSV dir: /content/drive/MyDrive/HealthTequity-LLM/data/synthetic_csv\n",
            "Audio dir: /content/drive/MyDrive/HealthTequity-LLM/data/Spanish_audio\n",
            "Results dir: /content/drive/MyDrive/HealthTequity-LLM/results\n",
            "LLM outputs: /content/drive/MyDrive/HealthTequity-LLM/results/llm_outputs\n",
            "Evaluation dir: /content/drive/MyDrive/HealthTequity-LLM/results/evaluation_metrics\n",
            "TTS dir: /content/drive/MyDrive/HealthTequity-LLM/results/tts_audio\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "# Project root in Google Drive (adjust if needed)\n",
        "PROJECT_ROOT = Path(\"/content/drive/MyDrive/HealthTequity-LLM\")\n",
        "\n",
        "# Data and results subfolders\n",
        "DATA_DIR     = PROJECT_ROOT / \"data\"\n",
        "CSV_DIR      = DATA_DIR / \"synthetic_csv\"\n",
        "AUDIO_DIR    = DATA_DIR / \"Spanish_audio\"\n",
        "RESULTS_DIR  = PROJECT_ROOT / \"results\"\n",
        "LLM_OUT      = RESULTS_DIR / \"llm_outputs\"\n",
        "EVAL_DIR     = RESULTS_DIR / \"evaluation_metrics\"\n",
        "TTS_DIR      = RESULTS_DIR / \"tts_audio\"\n",
        "\n",
        "# Create outputs if missing (idempotent)\n",
        "for p in [RESULTS_DIR, LLM_OUT, EVAL_DIR, TTS_DIR]:\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"Project root:\", PROJECT_ROOT)\n",
        "print(\"Data dir:\", DATA_DIR)\n",
        "print(\"CSV dir:\", CSV_DIR)\n",
        "print(\"Audio dir:\", AUDIO_DIR)\n",
        "print(\"Results dir:\", RESULTS_DIR)\n",
        "print(\"LLM outputs:\", LLM_OUT)\n",
        "print(\"Evaluation dir:\", EVAL_DIR)\n",
        "print(\"TTS dir:\", TTS_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "daf53a53",
      "metadata": {
        "id": "daf53a53"
      },
      "source": [
        "\n",
        "## OpenAI Key Initialization <a id=\"openai\"></a>\n",
        "This cell securely initializes the OpenAI client. If the `OPENAI_API_KEY` environment variable is not present, the cell prompts for a key using a hidden input.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "d2147149",
      "metadata": {
        "id": "d2147149",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "853812d6-ac04-444e-ae74-615f5da71a2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI client initialized.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "from getpass import getpass\n",
        "from openai import OpenAI\n",
        "\n",
        "# Attempt to load key from environment. Prompt if missing.\n",
        "if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "    print(\"OpenAI API key not found in environment.\")\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Paste your OpenAI API key (input hidden): \").strip()\n",
        "\n",
        "# Initialize client (will raise if key invalid)\n",
        "client = OpenAI()\n",
        "print(\"OpenAI client initialized.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2ead9ad",
      "metadata": {
        "id": "f2ead9ad"
      },
      "source": [
        "\n",
        "## Step 1 – ASR and Translation <a id=\"asr-translation\"></a>\n",
        "This section performs automatic speech recognition (ASR) on Spanish audio using Whisper (**base** model), followed by English translation via the OpenAI API.\n",
        "\n",
        "**Inputs**: `.wav` files under `AUDIO_DIR`.  \n",
        "**Outputs**: `audio_translations.csv` with columns: `audio_file`, `spanish_transcription`, `english_translation`, `language_detected`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "0128eb68",
      "metadata": {
        "id": "0128eb68"
      },
      "outputs": [],
      "source": [
        "\n",
        "import whisper\n",
        "import pandas as pd\n",
        "\n",
        "def transcribe_spanish_audio(model, audio_path: Path):\n",
        "    \"\"\"\n",
        "    Transcribe a single Spanish audio file using Whisper.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : whisper.Whisper\n",
        "        Loaded Whisper model instance.\n",
        "    audio_path : Path\n",
        "        Path to the .wav file.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    text : str\n",
        "        Detected transcription text.\n",
        "    lang : str\n",
        "        Detected language code.\n",
        "    \"\"\"\n",
        "    result = model.transcribe(str(audio_path), language=\"es\", task=\"transcribe\", verbose=False)\n",
        "    return result[\"text\"].strip(), result.get(\"language\", \"unknown\")\n",
        "\n",
        "\n",
        "def translate_spanish_to_english(spanish_text: str) -> str:\n",
        "    \"\"\"\n",
        "    Translate a Spanish transcription to English via OpenAI chat completion.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    spanish_text : str\n",
        "        Input Spanish text to translate.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    english_text : str\n",
        "        Translated English text.\n",
        "    \"\"\"\n",
        "    prompt = (\n",
        "        \"Translate the following Spanish medical transcription into clear, faithful English:\\n\\n\"\n",
        "        + spanish_text\n",
        "    )\n",
        "    result = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        temperature=0,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "    )\n",
        "    return result.choices[0].message.content.strip()\n",
        "\n",
        "\n",
        "def process_and_translate_audio(audio_folder: Path, output_csv: Path, model_size: str = \"base\") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Run Whisper ASR on all .wav files in a folder, translate to English, and save results.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    audio_folder : Path\n",
        "        Directory containing .wav files.\n",
        "    output_csv : Path\n",
        "        Destination CSV for transcriptions and translations.\n",
        "    model_size : str, optional\n",
        "        Whisper model size (default: \"base\").\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        DataFrame of results with columns:\n",
        "        [audio_file, spanish_transcription, english_translation, language_detected].\n",
        "    \"\"\"\n",
        "    model = whisper.load_model(model_size)\n",
        "    audio_files = sorted([f for f in os.listdir(audio_folder) if f.endswith(\".wav\")])\n",
        "\n",
        "    results = []\n",
        "    for fname in audio_files:\n",
        "        audio_path = audio_folder / fname\n",
        "        if not audio_path.exists():\n",
        "            continue\n",
        "        es_text, detected_lang = transcribe_spanish_audio(model, audio_path)\n",
        "        en_text = translate_spanish_to_english(es_text)\n",
        "        results.append({\n",
        "            \"audio_file\": fname,\n",
        "            \"spanish_transcription\": es_text,\n",
        "            \"english_translation\": en_text,\n",
        "            \"language_detected\": detected_lang\n",
        "        })\n",
        "    df = pd.DataFrame(results)\n",
        "    df.to_csv(output_csv, index=False, encoding=\"utf-8-sig\")\n",
        "    print(\"Saved:\", output_csv)\n",
        "    return df\n",
        "\n",
        "# Example (do not auto-run):\n",
        "# trans_csv = LLM_OUT / \"audio_translations.csv\"\n",
        "# _ = process_and_translate_audio(AUDIO_DIR, trans_csv, model_size=\"base\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c531344",
      "metadata": {
        "id": "8c531344"
      },
      "source": [
        "\n",
        "## Step 2 – ASR Evaluation (Input) <a id=\"asr-eval\"></a>\n",
        "This section computes WER and CER for the input ASR stage by aligning Whisper transcriptions with ground-truth text.\n",
        "\n",
        "**Assumptions**  \n",
        "- Ground truth is provided in `CSV_DIR / \"ground_truth.csv\"` with columns: `audio_file`, `ground_truth`.\n",
        "- Transcriptions are in `LLM_OUT / \"audio_translations.csv\"` with columns: `audio_file`, `spanish_transcription`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "24d16763",
      "metadata": {
        "id": "24d16763"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "from jiwer import wer, cer\n",
        "\n",
        "def evaluate_asr_performance(ground_truth_csv: Path, trans_csv: Path, save_csv: Path) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Compute WER and CER for input ASR against ground truth.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    ground_truth_csv : Path\n",
        "        CSV file with columns: [audio_file, ground_truth].\n",
        "    trans_csv : Path\n",
        "        CSV file with columns: [audio_file, spanish_transcription].\n",
        "    save_csv : Path\n",
        "        Destination CSV for ASR metrics.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        Evaluation results with per-file WER/CER.\n",
        "    \"\"\"\n",
        "    gt_df = pd.read_csv(ground_truth_csv)\n",
        "    tr_df = pd.read_csv(trans_csv)\n",
        "\n",
        "    # Defensive renaming for common variations.\n",
        "    gt_df = gt_df.rename(columns={\"filename\": \"audio_file\", \"spanish_text\": \"ground_truth\"})\n",
        "    tr_df = tr_df.rename(columns={\"transcription\": \"spanish_transcription\"})\n",
        "\n",
        "    df = pd.merge(tr_df[[\"audio_file\", \"spanish_transcription\"]],\n",
        "                  gt_df[[\"audio_file\", \"ground_truth\"]],\n",
        "                  on=\"audio_file\", how=\"inner\")\n",
        "\n",
        "    df[\"WER\"] = [wer(ref, hyp) for ref, hyp in zip(df[\"ground_truth\"], df[\"spanish_transcription\"])]\n",
        "    df[\"CER\"] = [cer(ref, hyp) for ref, hyp in zip(df[\"ground_truth\"], df[\"spanish_transcription\"])]\n",
        "\n",
        "    df.to_csv(save_csv, index=False)\n",
        "    print(\"Saved:\", save_csv)\n",
        "    return df\n",
        "\n",
        "# Example (do not auto-run):\n",
        "# gt_csv  = CSV_DIR / \"ground_truth.csv\"\n",
        "# trans_csv = LLM_OUT / \"audio_translations.csv\"\n",
        "# asr_csv = EVAL_DIR / \"asr_metrics.csv\"\n",
        "# _ = evaluate_asr_performance(gt_csv, trans_csv, asr_csv)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4477ecb",
      "metadata": {
        "id": "f4477ecb"
      },
      "source": [
        "\n",
        "## Step 3 – LLM Question Answering <a id=\"qa\"></a>\n",
        "This section queries an LLM with English questions derived from the ASR+translation step and provides answers based on a tabular blood-pressure dataset.\n",
        "\n",
        "**Inputs**: A CSV file with synthetic blood-pressure records.  \n",
        "**Outputs**: English answers and associated computed fields.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "85f89584",
      "metadata": {
        "id": "85f89584"
      },
      "outputs": [],
      "source": [
        "\n",
        "def ask_gpt(question_en: str, csv_block: str) -> dict:\n",
        "    \"\"\"\n",
        "    Query the LLM with a question and a CSV context block.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    question_en : str\n",
        "        English question derived from Spanish transcription.\n",
        "    csv_block : str\n",
        "        CSV content as a single string for in-context grounding.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        A dictionary with keys:\n",
        "        - \"answer\": str (LLM's English answer)\n",
        "        - \"computed_fields\": dict (optional structured fields)\n",
        "    \"\"\"\n",
        "\n",
        "    system = \"\"\"\n",
        "You are a careful and detail-oriented data analyst.\n",
        "\n",
        "You are given a synthetic blood pressure dataset in CSV format. It contains readings for one individual over the last 30 consecutive days, with the following columns:\n",
        "\n",
        "- date\n",
        "- age\n",
        "- sex\n",
        "- systolic_mmHg\n",
        "- diastolic_mmHg\n",
        "\n",
        "Use only the data in the CSV to answer all questions, except when normal blood pressure ranges are requested — in those cases, you may use external references but must cite your source.\n",
        "\n",
        "---\n",
        "\n",
        "🧠 Interpretation Guidelines:\n",
        "\n",
        "- \"Today\" refers to the most recent date in the dataset.\n",
        "- \"Yesterday\" means the most recent date before \"today\", based on available data.\n",
        "- Phrases like \"last week\" or \"last month\" refer to calendar-based timeframes (e.g., the 7 or 30 days before \"today\"), not just row counts.\n",
        "- If a question refers to a specific date or date range that is not present in the dataset, clearly state that the data is unavailable.\n",
        "- Use conversational date formats like “October 12” or “October 12 to 15” — avoid numeric formats like “10/12/2025”.\n",
        "\n",
        "---\n",
        "\n",
        "💬 Answer Style:\n",
        "\n",
        "- Write in natural, conversational English.\n",
        "- Address the user directly using “you” (e.g., “Your blood pressure was…”).\n",
        "\n",
        "---\n",
        "\n",
        "✅ Response Format:\n",
        "\n",
        "Return all answers in the following JSON format:\n",
        "\n",
        "{ \"answer\": \"<English answer>\", \"computed_fields\": { \"numeric values used\" } }\n",
        "\n",
        "---\n",
        "\n",
        "📌 Example Questions and Answers:\n",
        "\n",
        "These illustrate tone and structure only. Actual answers must be based on the CSV data.\n",
        "\n",
        "Q: What are my systolic_mmHg and diastolic_mmHg blood pressures today?\n",
        "A: Your systolic blood pressure was [xx] mm Hg and your diastolic pressure was [yy] mm Hg today.\n",
        "\n",
        "Q: What were the values over the last week?\n",
        "A: Over the last 7 days, your systolic pressure averaged [xx] mm Hg and your diastolic pressure averaged [yy] mm Hg.\n",
        "\n",
        "Q: What is the trend of my blood pressure?\n",
        "A: Your blood pressure has shown a gradual increase in systolic values over the last 30 days, while your diastolic readings have remained stable.\n",
        "\n",
        "Q: What are the normal ranges for a person like me?\n",
        "A: Based on your age ([age from dataset] years) and sex ([male/female]), typical blood pressure values are approximately [xx/yy] mm Hg, according to [name and link to the external source].\n",
        "\"\"\"\n",
        "\n",
        "    user = (\n",
        "        f\"CSV:\\n{csv_block}\\n\\n\"\n",
        "        f\"Question:\\n{question_en}\\n\\n\"\n",
        "        \"Please analyze and return the response strictly following the JSON format defined above.\"\n",
        "    )\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        temperature=0,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system},\n",
        "            {\"role\": \"user\", \"content\": user},\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    answer_text = response.choices[0].message.content.strip()\n",
        "\n",
        "    # Ensure output is JSON-safe\n",
        "    try:\n",
        "        result = json.loads(answer_text)\n",
        "    except json.JSONDecodeError:\n",
        "        result = {\"answer\": answer_text, \"computed_fields\": {}}\n",
        "\n",
        "    return result\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b90c1a9b",
      "metadata": {
        "id": "b90c1a9b"
      },
      "source": [
        "\n",
        "## Step 4 – Translation and TTS <a id=\"tts\"></a>\n",
        "This section back-translates the LLM's English answers to Spanish and generates Spanish audio (TTS).\n",
        "\n",
        "Note: A simple gTTS-based fallback is provided (exports `.wav` via pydub). If your project includes a custom TTS, replace the fallback with your implementation and keep the same function signature.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "589d5361",
      "metadata": {
        "id": "589d5361"
      },
      "outputs": [],
      "source": [
        "\n",
        "from pathlib import Path\n",
        "\n",
        "def translate_to_spanish(text_en: str) -> str:\n",
        "    \"\"\"\n",
        "    Translate English text to Spanish using OpenAI chat completion.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    text_en : str\n",
        "        English text to translate.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        Spanish translation.\n",
        "    \"\"\"\n",
        "    prompt = \"Translate the following English medical answer into Spanish:\\n\\n\" + text_en\n",
        "    resp = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        temperature=0,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "    )\n",
        "    return resp.choices[0].message.content.strip()\n",
        "\n",
        "\n",
        "def text_to_speech_spanish(text_es: str, out_wav_path: Path):\n",
        "    \"\"\"\n",
        "    Generate Spanish speech from text.\n",
        "\n",
        "    This fallback uses gTTS + pydub to export a WAV file if a custom TTS is\n",
        "    not available. Replace this function with your project-specific TTS if needed.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    text_es : str\n",
        "        Spanish text to synthesize.\n",
        "    out_wav_path : Path\n",
        "        Destination path for the WAV file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from gtts import gTTS\n",
        "        from pydub import AudioSegment\n",
        "        tmp_mp3 = out_wav_path.with_suffix(\".mp3\")\n",
        "        gTTS(text_es, lang=\"es\").save(tmp_mp3)\n",
        "        # Convert to WAV\n",
        "        audio = AudioSegment.from_file(tmp_mp3, format=\"mp3\")\n",
        "        audio.export(out_wav_path, format=\"wav\")\n",
        "        os.remove(tmp_mp3)\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"TTS fallback failed: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af0262a4",
      "metadata": {
        "id": "af0262a4"
      },
      "source": [
        "\n",
        "## Step 5 – Whisper Evaluation of TTS <a id=\"tts-eval\"></a>\n",
        "This section re-transcribes the generated Spanish audio responses using Whisper (base) and evaluates intelligibility against the ground-truth Spanish answers using WER, CER, and SER.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "f48ecab5",
      "metadata": {
        "id": "f48ecab5"
      },
      "outputs": [],
      "source": [
        "\n",
        "import re, unicodedata\n",
        "import Levenshtein\n",
        "from jiwer import process_words\n",
        "\n",
        "def normalize_text(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Normalize text for fair ASR comparison.\n",
        "    - Lowercase\n",
        "    - Strip accents\n",
        "    - Remove punctuation\n",
        "    - Collapse extra whitespace\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    text = ''.join(c for c in unicodedata.normalize('NFD', text) if unicodedata.category(c) != 'Mn')\n",
        "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "\n",
        "def evaluate_output_asr_whisper(\n",
        "    tts_csv: Path,\n",
        "    output_csv: Path = None,\n",
        "    model_size: str = \"base\"\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Evaluate Spanish TTS audios using Whisper by computing WER, CER, and SER\n",
        "    against ground-truth Spanish answers.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    tts_csv : Path\n",
        "        CSV with columns: [spanish_answer, audio_answer_file].\n",
        "    output_csv : Path, optional\n",
        "        Destination CSV for ASR metrics (defaults to EVAL_DIR / \"output_asr_metrics_whisper.csv\").\n",
        "    model_size : str, optional\n",
        "        Whisper model size (default: \"base\").\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        DataFrame with per-file metrics.\n",
        "    \"\"\"\n",
        "    if output_csv is None:\n",
        "        output_csv = EVAL_DIR / \"output_asr_metrics_whisper.csv\"\n",
        "\n",
        "    if not tts_csv.exists():\n",
        "        raise FileNotFoundError(f\"Missing final results CSV: {tts_csv}\")\n",
        "\n",
        "    model = whisper.load_model(model_size)\n",
        "\n",
        "    df = pd.read_csv(tts_csv)\n",
        "    rows = []\n",
        "    for _, row in df.iterrows():\n",
        "        gt = str(row.get(\"spanish_answer\", \"\")).strip()\n",
        "        audio_file = str(row.get(\"audio_answer_file\", \"\")).strip()\n",
        "        if not gt or not audio_file or not os.path.exists(audio_file):\n",
        "            continue\n",
        "\n",
        "        # Transcribe generated audio and normalize\n",
        "        res = model.transcribe(audio_file, language=\"es\", task=\"transcribe\", verbose=False)\n",
        "        hyp = res.get(\"text\", \"\").strip()\n",
        "\n",
        "        gt_norm = normalize_text(gt)\n",
        "        hyp_norm = normalize_text(hyp)\n",
        "\n",
        "        measures = process_words(gt_norm, hyp_norm)\n",
        "        wer_score = round(measures.wer, 4)\n",
        "        subs, dels, ins = measures.substitutions, measures.deletions, measures.insertions\n",
        "        cer_score = round(Levenshtein.distance(gt_norm, hyp_norm) / max(len(gt_norm), 1), 4)\n",
        "        ser_score = 0 if gt_norm == hyp_norm else 1\n",
        "\n",
        "        rows.append({\n",
        "            \"audio_file\": os.path.basename(audio_file),\n",
        "            \"ground_truth\": gt,\n",
        "            \"whisper_transcription\": hyp,\n",
        "            \"WER\": wer_score,\n",
        "            \"Substitutions\": subs,\n",
        "            \"Deletions\": dels,\n",
        "            \"Insertions\": ins,\n",
        "            \"CER\": cer_score,\n",
        "            \"SER\": ser_score,\n",
        "        })\n",
        "\n",
        "    out_df = pd.DataFrame(rows)\n",
        "    out_df.to_csv(output_csv, index=False)\n",
        "    print(\"Saved:\", output_csv)\n",
        "    return out_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9911afe5",
      "metadata": {
        "id": "9911afe5"
      },
      "source": [
        "\n",
        "## Orchestration (Optional) <a id=\"orchestration\"></a>\n",
        "The following function orchestrates all steps in sequence. Each step can also be run individually.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "852067e6",
      "metadata": {
        "id": "852067e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "017889af-492a-4531-880a-db3a3c9a7734"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "100%|██████████| 458/458 [00:01<00:00, 242.60frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "100%|██████████| 470/470 [00:01<00:00, 266.51frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "100%|██████████| 369/369 [00:01<00:00, 209.37frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "100%|██████████| 398/398 [00:01<00:00, 212.16frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "100%|██████████| 328/328 [00:01<00:00, 191.48frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "100%|██████████| 1574/1574 [00:02<00:00, 558.36frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: /content/drive/MyDrive/HealthTequity-LLM/results/llm_outputs/audio_translations.csv\n",
            "Saved: /content/drive/MyDrive/HealthTequity-LLM/results/evaluation_metrics/asr_metrics.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pydub/utils.py:300: SyntaxWarning: invalid escape sequence '\\('\n",
            "  m = re.match('([su]([0-9]{1,2})p?) \\(([0-9]{1,2}) bit\\)$', token)\n",
            "/usr/local/lib/python3.12/dist-packages/pydub/utils.py:301: SyntaxWarning: invalid escape sequence '\\('\n",
            "  m2 = re.match('([su]([0-9]{1,2})p?)( \\(default\\))?$', token)\n",
            "/usr/local/lib/python3.12/dist-packages/pydub/utils.py:310: SyntaxWarning: invalid escape sequence '\\('\n",
            "  elif re.match('(flt)p?( \\(default\\))?$', token):\n",
            "/usr/local/lib/python3.12/dist-packages/pydub/utils.py:314: SyntaxWarning: invalid escape sequence '\\('\n",
            "  elif re.match('(dbl)p?( \\(default\\))?$', token):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: /content/drive/MyDrive/HealthTequity-LLM/results/llm_outputs/final_pipeline_results.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "100%|██████████| 1099/1099 [00:02<00:00, 438.77frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "100%|██████████| 1305/1305 [00:03<00:00, 357.46frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "100%|██████████| 4099/4099 [00:06<00:00, 648.85frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "100%|██████████| 1790/1790 [00:04<00:00, 413.96frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "100%|██████████| 1221/1221 [00:02<00:00, 514.08frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "100%|██████████| 6192/6192 [00:09<00:00, 655.82frames/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: /content/drive/MyDrive/HealthTequity-LLM/results/evaluation_metrics/output_asr_metrics_whisper.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "def run_full_pipeline(csv_path: Path, audio_folder: Path, whisper_model_size: str = \"base\"):\n",
        "    \"\"\"\n",
        "    Execute the full pipeline, from input ASR/translation to TTS evaluation.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    csv_path : Path\n",
        "        Path to the synthetic blood-pressure CSV.\n",
        "    audio_folder : Path\n",
        "        Directory containing input Spanish .wav files.\n",
        "    whisper_model_size : str, optional\n",
        "        Whisper model size, default \"base\".\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        Summary dictionary with output artifact locations.\n",
        "    \"\"\"\n",
        "    # Step 1: ASR + Translation\n",
        "    trans_csv = LLM_OUT / \"audio_translations.csv\"\n",
        "    _ = process_and_translate_audio(audio_folder, trans_csv, model_size=whisper_model_size)\n",
        "\n",
        "    # Step 2: Evaluate input ASR\n",
        "    gt_csv = CSV_DIR / \"ground_truth.csv\"\n",
        "    asr_csv = EVAL_DIR / \"asr_metrics.csv\"\n",
        "    _ = evaluate_asr_performance(gt_csv, trans_csv, asr_csv)\n",
        "\n",
        "    # Step 3: Load tabular data for LLM grounding\n",
        "    df_bp = pd.read_csv(csv_path)\n",
        "    csv_block = df_bp.to_csv(index=False)\n",
        "\n",
        "    # Step 4: Q&A + Spanish TTS\n",
        "    results = []\n",
        "    tr_df = pd.read_csv(trans_csv)\n",
        "    for i, row in tr_df.iterrows():\n",
        "        q_en = row[\"english_translation\"]\n",
        "        ans = ask_gpt(q_en, csv_block)\n",
        "        ans_en = ans.get(\"answer\", \"\").strip()\n",
        "        ans_es = translate_to_spanish(ans_en)\n",
        "\n",
        "        out_wav = TTS_DIR / f\"answer_{i+1}_es.wav\"\n",
        "        text_to_speech_spanish(ans_es, out_wav)\n",
        "\n",
        "        results.append({\n",
        "            \"question_number\": i + 1,\n",
        "            \"audio_file_in\": row[\"audio_file\"],\n",
        "            \"spanish_question\": row[\"spanish_transcription\"],\n",
        "            \"english_question\": q_en,\n",
        "            \"english_answer\": ans_en,\n",
        "            \"spanish_answer\": ans_es,\n",
        "            \"audio_answer_file\": str(out_wav),\n",
        "            \"computed_fields\": json.dumps(ans.get(\"computed_fields\", {}))\n",
        "        })\n",
        "\n",
        "    final_csv = LLM_OUT / \"final_pipeline_results.csv\"\n",
        "    pd.DataFrame(results).to_csv(final_csv, index=False)\n",
        "    print(\"Saved:\", final_csv)\n",
        "\n",
        "    # Step 5: Evaluate TTS intelligibility\n",
        "    output_asr_csv = EVAL_DIR / \"output_asr_metrics_whisper.csv\"\n",
        "    _ = evaluate_output_asr_whisper(final_csv, output_csv=output_asr_csv, model_size=whisper_model_size)\n",
        "\n",
        "    return {\n",
        "        \"transcriptions_csv\": str(trans_csv),\n",
        "        \"input_asr_metrics_csv\": str(asr_csv),\n",
        "        \"final_pipeline_csv\": str(final_csv),\n",
        "        \"output_asr_metrics_csv\": str(output_asr_csv),\n",
        "    }\n",
        "\n",
        "# Example (do not auto-run):\n",
        "bp_csv = CSV_DIR / \"synthetic_bp_one_person.csv\"\n",
        "_ = run_full_pipeline(bp_csv, AUDIO_DIR, whisper_model_size=\"base\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b1fc405",
      "metadata": {
        "id": "5b1fc405"
      },
      "source": [
        "\n",
        "## Step 6 – Results Summary <a id=\"summary\"></a>\n",
        "This section summarizes average WER, CER, and SER across input ASR and TTS evaluation outputs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "23c1ed41",
      "metadata": {
        "id": "23c1ed41"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "def summarize_results(input_asr_csv: Path, output_asr_csv: Path):\n",
        "    \"\"\"\n",
        "    Print dataset-level average metrics for input ASR and TTS ASR evaluation.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_asr_csv : Path\n",
        "        CSV with per-file input ASR metrics.\n",
        "    output_asr_csv : Path\n",
        "        CSV with per-file TTS ASR metrics.\n",
        "    \"\"\"\n",
        "    print(\"Input ASR metrics (WER, CER):\")\n",
        "    if Path(input_asr_csv).exists():\n",
        "        d1 = pd.read_csv(input_asr_csv)\n",
        "        print(d1[[\"WER\", \"CER\", \"SER\"]].mean(numeric_only=True).to_frame(\"Average\"))\n",
        "    else:\n",
        "        print(\"Missing:\", input_asr_csv)\n",
        "\n",
        "    print(\"\\nTTS ASR metrics (WER, CER, SER):\")\n",
        "    if Path(output_asr_csv).exists():\n",
        "        d2 = pd.read_csv(output_asr_csv)\n",
        "        print(d2[[\"WER\", \"CER\", \"SER\"]].mean(numeric_only=True).to_frame(\"Average\"))\n",
        "    else:\n",
        "        print(\"Missing:\", output_asr_csv)\n",
        "\n",
        "# Example (do not auto-run):\n",
        "# summarize_results(EVAL_DIR / \"asr_metrics.csv\", EVAL_DIR / \"output_asr_metrics_whisper.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "summarize_results(EVAL_DIR / \"asr_metrics.csv\", EVAL_DIR / \"output_asr_metrics_whisper.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jXT7dn1Nz8D",
        "outputId": "7704d700-9e0c-41a6-ff15-71acccf9db5e"
      },
      "id": "1jXT7dn1Nz8D",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input ASR metrics (WER, CER):\n",
            "      Average\n",
            "WER  0.035183\n",
            "CER  0.005517\n",
            "SER  0.333333\n",
            "\n",
            "TTS ASR metrics (WER, CER, SER):\n",
            "      Average\n",
            "WER  0.211000\n",
            "CER  0.160367\n",
            "SER  1.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c776ee9f",
      "metadata": {
        "id": "c776ee9f"
      },
      "source": [
        "\n",
        "## Appendix <a id=\"appendix\"></a>\n",
        "- All paths are centralized under `PROJECT_ROOT` for reproducibility.  \n",
        "- Replace the TTS fallback with a project-specific implementation if available.  \n",
        "- The Whisper model size can be adjusted by changing `whisper_model_size` in the orchestration call.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}