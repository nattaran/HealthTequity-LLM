{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nattaran/HealthTequity-LLM/blob/main/HealthTequity_VoicePipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🩺 HealthTequity Voice Processing Pipeline\n",
        "\n",
        "## 📘 Introduction\n",
        "This notebook demonstrates an end-to-end **Voice-to-Insight** pipeline developed for the **HealthTequity Case Study**.  \n",
        "It processes Spanish medical speech into actionable insights through **transcription**, **translation**, **LLM-based reasoning**, and **automated evaluation** using **WER**, **CER**, and **SER** metrics.  \n",
        "\n",
        "The system connects **speech understanding (ASR)** with **data analytics (LLM)** and **speech synthesis (TTS)**, forming a reproducible and modular workflow for healthcare data analysis.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧩 Companion Notebooks\n",
        "Two supporting notebooks generate the datasets used in this pipeline:\n",
        "\n",
        "| Notebook | Purpose | Output Folder |\n",
        "|-----------|----------|---------------|\n",
        "| **Synthetic Blood Pressure Generator** | Creates a 30-day synthetic blood pressure dataset | `data/synthetic_csv/` |\n",
        "| **Spanish Audio Generator** | Produces Spanish-language health questions from the dataset | `data/Spanish_audio/` |\n",
        "\n",
        "> 🔹 *Reviewers may also upload their own CSVs and audio recordings.*  \n",
        "> To compute **WER/CER/SER**, ensure a `ground_truth.csv` file exists with reference text for each input audio file.\n",
        "\n",
        "---\n",
        "\n",
        "### ⚙️ Pipeline Overview\n",
        "\n",
        "| Step | Component | Description | Output |\n",
        "|------|------------|-------------|---------|\n",
        "| **1️⃣** | **ASR (Whisper)** | Transcribes Spanish medical audio into text | Spanish transcript |\n",
        "| **2️⃣** | **Translation (GPT)** | Translates Spanish → English for question understanding | `audio_translations.csv` |\n",
        "| **3️⃣** | **ASR Evaluation (Input)** | Compares transcriptions to ground truth → computes **WER**, **CER**, **SER** | `input_asr_metrics.csv` |\n",
        "| **4️⃣** | **LLM Analysis (GPT-4o-mini)** | Answers English questions using the blood pressure CSV context | `final_pipeline_results.csv` |\n",
        "| **5️⃣** | **Spanish TTS (gTTS)** | Converts English answers back into spoken Spanish | `tts_audio/*.wav` |\n",
        "| **6️⃣** | **ASR Evaluation (Output)** | Evaluates TTS intelligibility via Whisper ASR (WER/CER/SER) | `output_asr_metrics.csv` |\n",
        "| **7️⃣** | **Visualization** | Plots a bar chart comparing input vs. output ASR metrics | `asr_comparison_chart.png` |\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Translation Toggle\n",
        "\n",
        "This pipeline can use either **Whisper** or **GPT** for Spanish-to-English translation:\n",
        "\n",
        "| Mode | How to Enable | Description |\n",
        "|------|----------------|-------------|\n",
        "| 🧩 **GPT Translation (Recommended)** | `USE_GPT_TRANSLATION = True` | Uses GPT-4o-mini for domain-accurate translations (preserves medical terms and numbers). Requires OpenAI API key. |\n",
        "| ⚙️ **Whisper-Only Mode (Free)** | `USE_GPT_TRANSLATION = False` | Uses Whisper’s built-in `task=\"translate\"` mode. Works offline but may reduce translation accuracy. |\n",
        "\n",
        "You can change this toggle at the top of the ASR module.\n",
        "\n",
        "---\n",
        "\n",
        "### 📊 Evaluation Metrics\n",
        "\n",
        "| Metric | Description | Interpretation |\n",
        "|---------|-------------|----------------|\n",
        "| **WER (Word Error Rate)** | Measures overall transcription accuracy (substitutions, deletions, insertions) | Lower = better |\n",
        "| **CER (Character Error Rate)** | Captures fine-grained textual differences | Sensitive to short utterances |\n",
        "| **SER (Sentence Error Rate)** | Binary metric — whether a sentence matches exactly | 0 = perfect, 1 = any error |\n",
        "\n",
        "All metrics are computed for both **input** (Spanish audio → text) and **output** (Spanish TTS → text) stages.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧰 System Notes\n",
        "- Default Whisper model is `\"base\"` but can be changed to `\"small\"`, `\"medium\"`, or `\"large\"`.  \n",
        "- All directories (`data/`, `results/`, etc.) are auto-created by the pipeline.  \n",
        "- Results and charts are saved under `/results/` for easy retrieval.  \n",
        "- The notebook can run entirely on Google Colab; only GPT translation requires an OpenAI API key.\n",
        "\n",
        "---\n",
        "\n",
        "### 📂 Folder Structure\n",
        "\n",
        "```text\n",
        "/content/drive/MyDrive/HealthTequity-LLM/\n",
        "│\n",
        "├── data/\n",
        "│   ├── synthetic_csv/              ← Synthetic BP data + ground truth\n",
        "│   │   ├── synthetic_bp_one_person.csv\n",
        "│   │   └── ground_truth.csv\n",
        "│   │\n",
        "│   └── Spanish_audio/              ← Input Spanish audio files\n",
        "│       ├── question_1_es.wav\n",
        "│       ├── question_2_es.wav\n",
        "│       └── ...\n",
        "│\n",
        "├── results/\n",
        "│   ├── llm_outputs/                ← Translations + LLM responses\n",
        "│   ├── tts_audio/                  ← Spanish audio answers\n",
        "│   └── evaluation_metrics/         ← WER/CER/SER + chart\n",
        "│\n",
        "├── BloodPressure_Generator.ipynb\n",
        "├── SpanishAudio_Generator.ipynb\n",
        "└── HealthTequity_VoicePipeline.ipynb\n"
      ],
      "metadata": {
        "id": "XBAdMOleww-o"
      },
      "id": "XBAdMOleww-o"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📁 Step 1 — Mount Google Drive and Sync Project Repository\n",
        "\n",
        "This step connects your Colab environment to Google Drive and ensures you have the latest version of the **HealthTequity-LLM** project.\n",
        "\n",
        "**What this does:**\n",
        "1. Clears any previously mounted Drive (prevents the “mountpoint already contains files” error).\n",
        "2. Mounts Google Drive at `/content/drive`.\n",
        "3. Clones the `HealthTequity-LLM` GitHub repository into your Drive if it doesn’t exist,  \n",
        "   or updates it with the latest version if it already exists.\n",
        "\n",
        "**After this cell runs successfully:**\n",
        "- You’ll be working directly from  \n",
        "  `/content/drive/MyDrive/HealthTequity-LLM`\n",
        "- All project data, code, and outputs will stay persistent in your Google Drive.\n"
      ],
      "metadata": {
        "id": "0pkaokMr0f7Q"
      },
      "id": "0pkaokMr0f7Q"
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# 📁 STEP 1 — Mount Google Drive and Clone/Update Repository\n",
        "# ==========================================================\n",
        "from google.colab import drive\n",
        "import os, shutil\n",
        "\n",
        "MOUNT_POINT = '/content/drive'\n",
        "REPO_URL = \"https://github.com/nattaran/HealthTequity-LLM.git\"\n",
        "REPO_PATH = f\"{MOUNT_POINT}/MyDrive/HealthTequity-LLM\"\n",
        "\n",
        "# --- Clean any existing mountpoint to prevent ValueError ---\n",
        "if os.path.exists(MOUNT_POINT) and os.path.isdir(MOUNT_POINT) and os.listdir(MOUNT_POINT):\n",
        "    print(f\"⚙️ Clearing existing mountpoint: {MOUNT_POINT}\")\n",
        "    try:\n",
        "        shutil.rmtree(MOUNT_POINT)\n",
        "        os.makedirs(MOUNT_POINT)\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Warning: Could not fully clear mountpoint: {e}\")\n",
        "\n",
        "# --- Mount Google Drive ---\n",
        "print(\"🔗 Mounting Google Drive...\")\n",
        "drive.mount(MOUNT_POINT, force_remount=True)\n",
        "\n",
        "# --- Clone or update GitHub repo ---\n",
        "if not os.path.exists(REPO_PATH):\n",
        "    print(f\"📦 Cloning repository into {REPO_PATH}...\")\n",
        "    !git clone {REPO_URL} {REPO_PATH}\n",
        "else:\n",
        "    print(\"🔄 Repository already exists — updating...\")\n",
        "    %cd {REPO_PATH}\n",
        "    !git fetch origin\n",
        "    !git pull\n",
        "\n",
        "%cd {REPO_PATH}\n",
        "print(f\"✅ Environment ready. Working directory: {os.getcwd()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9GGDiUrHnS2",
        "outputId": "aef57360-f19f-4b16-c710-5150f9b5e1bb"
      },
      "id": "R9GGDiUrHnS2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔗 Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "🔄 Repository already exists — updating...\n",
            "/content/drive/MyDrive/HealthTequity-LLM\n",
            "remote: Enumerating objects: 5, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 3 (delta 2), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Unpacking objects: 100% (3/3), 12.41 KiB | 181.00 KiB/s, done.\n",
            "From https://github.com/nattaran/HealthTequity-LLM\n",
            "   645c368..0dd138f  main       -> origin/main\n",
            "Updating 645c368..0dd138f\n",
            "Fast-forward\n",
            " HealthTequity_VoicePipeline.ipynb | 1354 \u001b[32m+++++++++++++++++++++++\u001b[m\u001b[31m--------------\u001b[m\n",
            " 1 file changed, 837 insertions(+), 517 deletions(-)\n",
            "/content/drive/MyDrive/HealthTequity-LLM\n",
            "✅ Environment ready. Working directory: /content/drive/MyDrive/HealthTequity-LLM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ⚙️ Step 2 — Install Project Dependencies\n",
        "\n",
        "This step installs all the required Python packages for the HealthTequity-LLM pipeline.\n",
        "\n",
        "**Notes:**\n",
        "- Run this cell once per Colab session.\n",
        "- Dependencies are listed in `requirements.txt` at the repository root.\n",
        "- You can add or pin package versions there (e.g., `whisper==1.0`, `jiwer==3.0.2`).\n",
        "- Colab may show warnings for already-installed packages — you can safely ignore them.\n"
      ],
      "metadata": {
        "id": "6wKfk2K51qpt"
      },
      "id": "6wKfk2K51qpt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a53aba9f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a53aba9f",
        "outputId": "c2034e65-43ac-4ba6-c3ce-e76fd9f7ad04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git (from -r requirements.txt (line 22))\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-w3s6_5tk\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-w3s6_5tk\n",
            "  Resolved https://github.com/openai/whisper.git to commit c0d2f624c09dc18e709e37c2ad90c039a4eb72a2\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (2.0.2)\n",
            "Requirement already satisfied: pandas>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 7)) (2.2.2)\n",
            "Requirement already satisfied: matplotlib>=3.8.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 8)) (3.10.0)\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 11)) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchaudio>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 12)) (2.8.0+cu126)\n",
            "Requirement already satisfied: openai>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 15)) (1.109.1)\n",
            "Requirement already satisfied: tiktoken>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 16)) (0.12.0)\n",
            "Collecting whisper (from -r requirements.txt (line 20))\n",
            "  Downloading whisper-1.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ffmpeg-python>=0.2.0 (from -r requirements.txt (line 25))\n",
            "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 26)) (0.13.1)\n",
            "Requirement already satisfied: librosa>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 27)) (0.11.0)\n",
            "Requirement already satisfied: pydub>=0.25.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 28)) (0.25.1)\n",
            "Collecting jiwer>=3.0.3 (from -r requirements.txt (line 31))\n",
            "  Downloading jiwer-4.0.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting python-Levenshtein>=0.25.0 (from -r requirements.txt (line 32))\n",
            "  Downloading python_levenshtein-0.27.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting gtts>=2.3.2 (from -r requirements.txt (line 35))\n",
            "  Downloading gTTS-2.5.4-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting deep-translator>=1.11.4 (from -r requirements.txt (line 36))\n",
            "  Downloading deep_translator-1.11.4-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: python-dotenv>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 39)) (1.1.1)\n",
            "Requirement already satisfied: tqdm>=4.66.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 40)) (4.67.1)\n",
            "Collecting vosk==0.3.45 (from -r requirements.txt (line 43))\n",
            "  Downloading vosk-0.3.45-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from vosk==0.3.45->-r requirements.txt (line 43)) (2.0.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from vosk==0.3.45->-r requirements.txt (line 43)) (2.32.4)\n",
            "Collecting srt (from vosk==0.3.45->-r requirements.txt (line 43))\n",
            "  Downloading srt-3.5.3.tar.gz (28 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: websockets in /usr/local/lib/python3.12/dist-packages (from vosk==0.3.45->-r requirements.txt (line 43)) (15.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.0->-r requirements.txt (line 7)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.0->-r requirements.txt (line 7)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.0->-r requirements.txt (line 7)) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8.0->-r requirements.txt (line 8)) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8.0->-r requirements.txt (line 8)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8.0->-r requirements.txt (line 8)) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8.0->-r requirements.txt (line 8)) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8.0->-r requirements.txt (line 8)) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8.0->-r requirements.txt (line 8)) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8.0->-r requirements.txt (line 8)) (3.2.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->-r requirements.txt (line 11)) (3.4.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.12.0->-r requirements.txt (line 15)) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.12.0->-r requirements.txt (line 15)) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.12.0->-r requirements.txt (line 15)) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.12.0->-r requirements.txt (line 15)) (0.11.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.12.0->-r requirements.txt (line 15)) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai>=1.12.0->-r requirements.txt (line 15)) (1.3.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken>=0.6.0->-r requirements.txt (line 16)) (2024.11.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from whisper->-r requirements.txt (line 20)) (1.17.0)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625->-r requirements.txt (line 22)) (10.8.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625->-r requirements.txt (line 22)) (0.60.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.12/dist-packages (from ffmpeg-python>=0.2.0->-r requirements.txt (line 25)) (1.0.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.10.1->-r requirements.txt (line 27)) (3.0.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.10.1->-r requirements.txt (line 27)) (1.16.2)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.10.1->-r requirements.txt (line 27)) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.10.1->-r requirements.txt (line 27)) (1.5.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.10.1->-r requirements.txt (line 27)) (4.4.2)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.10.1->-r requirements.txt (line 27)) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.10.1->-r requirements.txt (line 27)) (1.0.0)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.10.1->-r requirements.txt (line 27)) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.10.1->-r requirements.txt (line 27)) (1.1.2)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.12/dist-packages (from jiwer>=3.0.3->-r requirements.txt (line 31)) (8.3.0)\n",
            "Collecting rapidfuzz>=3.9.7 (from jiwer>=3.0.3->-r requirements.txt (line 31))\n",
            "  Downloading rapidfuzz-3.14.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\n",
            "Collecting Levenshtein==0.27.1 (from python-Levenshtein>=0.25.0->-r requirements.txt (line 32))\n",
            "  Downloading levenshtein-0.27.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting click>=8.1.8 (from jiwer>=3.0.3->-r requirements.txt (line 31))\n",
            "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.12/dist-packages (from deep-translator>=1.11.4->-r requirements.txt (line 36)) (4.13.5)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai>=1.12.0->-r requirements.txt (line 15)) (3.11)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator>=1.11.4->-r requirements.txt (line 36)) (2.8)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->vosk==0.3.45->-r requirements.txt (line 43)) (2.23)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai>=1.12.0->-r requirements.txt (line 15)) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai>=1.12.0->-r requirements.txt (line 15)) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.12.0->-r requirements.txt (line 15)) (0.16.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->openai-whisper==20250625->-r requirements.txt (line 22)) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa>=0.10.1->-r requirements.txt (line 27)) (4.5.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai>=1.12.0->-r requirements.txt (line 15)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai>=1.12.0->-r requirements.txt (line 15)) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai>=1.12.0->-r requirements.txt (line 15)) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->vosk==0.3.45->-r requirements.txt (line 43)) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->vosk==0.3.45->-r requirements.txt (line 43)) (2.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1.0->librosa>=0.10.1->-r requirements.txt (line 27)) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.1.0->-r requirements.txt (line 11)) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.1.0->-r requirements.txt (line 11)) (3.0.3)\n",
            "Downloading vosk-0.3.45-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (7.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
            "Downloading jiwer-4.0.0-py3-none-any.whl (23 kB)\n",
            "Downloading python_levenshtein-0.27.1-py3-none-any.whl (9.4 kB)\n",
            "Downloading levenshtein-0.27.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (159 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gTTS-2.5.4-py3-none-any.whl (29 kB)\n",
            "Downloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click-8.1.8-py3-none-any.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidfuzz-3.14.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: whisper, openai-whisper, srt\n",
            "  Building wheel for whisper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for whisper: filename=whisper-1.1.10-py3-none-any.whl size=41120 sha256=844b30148565635d971f7d1150861477089a2c6dbf052350f723390c3d80160f\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/b8/4e/9c4c3351d670e06746a340fb4b7d854c76517eec225e5b32b1\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20250625-py3-none-any.whl size=803979 sha256=637c23df617fa0c27d36abaec3860a68d685d234f2dfe58b4f5bd0ed7d0e054a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-8pwcn7ll/wheels/c3/03/25/5e0ba78bc27a3a089f137c9f1d92fdfce16d06996c071a016c\n",
            "  Building wheel for srt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for srt: filename=srt-3.5.3-py3-none-any.whl size=22427 sha256=43e155a24de5ad725b4b27f22421ad899e1dd79179c9f9fab0a60236be90a8ea\n",
            "  Stored in directory: /root/.cache/pip/wheels/7e/75/5b/e1d5c3756631e4bda806f6cc9640153b39484bb6f7b0b8def3\n",
            "Successfully built whisper openai-whisper srt\n",
            "Installing collected packages: whisper, srt, rapidfuzz, ffmpeg-python, click, vosk, Levenshtein, jiwer, gtts, deep-translator, python-Levenshtein, openai-whisper\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.3.0\n",
            "    Uninstalling click-8.3.0:\n",
            "      Successfully uninstalled click-8.3.0\n",
            "Successfully installed Levenshtein-0.27.1 click-8.1.8 deep-translator-1.11.4 ffmpeg-python-0.2.0 gtts-2.5.4 jiwer-4.0.0 openai-whisper-20250625 python-Levenshtein-0.27.1 rapidfuzz-3.14.1 srt-3.5.3 vosk-0.3.45 whisper-1.1.10\n",
            "\n",
            "✅ Package installation complete.\n",
            "Python version: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
            "geopandas                                1.1.1\n",
            "jiwer                                    4.0.0\n",
            "matplotlib                               3.10.0\n",
            "matplotlib-inline                        0.1.7\n",
            "matplotlib-venn                          1.1.2\n",
            "openai                                   1.109.1\n",
            "openai-whisper                           20250625\n",
            "pandas                                   2.2.2\n",
            "pandas-datareader                        0.10.0\n",
            "pandas-gbq                               0.29.2\n",
            "pandas-stubs                             2.2.2.240909\n",
            "sklearn-pandas                           2.2.0\n",
            "torch                                    2.8.0+cu126\n",
            "torchao                                  0.10.0\n",
            "torchaudio                               2.8.0+cu126\n",
            "torchdata                                0.11.0\n",
            "torchsummary                             1.5.1\n",
            "torchtune                                0.6.1\n",
            "torchvision                              0.23.0+cu126\n",
            "whisper                                  1.1.10\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Install project requirements (run once per session)\n",
        "# If you prefer to pin versions, ensure requirements.txt includes exact versions.\n",
        "!pip install -r requirements.txt\n",
        "# Verify installation of key modules\n",
        "import sys\n",
        "print(\"\\n✅ Package installation complete.\")\n",
        "print(\"Python version:\", sys.version)\n",
        "!pip list | grep -E \"openai|whisper|jiwer|pandas|torch|matplotlib\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📂 Step 3 — Project Paths and Data Dependencies\n",
        "\n",
        "This step defines the directory structure used by the **HealthTequity Voice Pipeline**  \n",
        "and ensures all necessary folders exist within your Google Drive.\n",
        "\n",
        "By default, the **Audio Generation Notebook** automatically produces:\n",
        "- Spanish question audio files (`.wav`) under `data/Spanish_audio/`\n",
        "- The corresponding `ground_truth.csv` file under `data/synthetic_csv/`\n",
        "- A synthetic blood-pressure dataset (`synthetic_bp_one_person.csv`) under the same folder\n",
        "\n",
        "These files are automatically detected when this pipeline runs.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧩 Reviewer Flexibility\n",
        "If you prefer to **test with your own data**, you can:\n",
        "- Upload custom `.wav` files to `data/Spanish_audio/`\n",
        "- Upload your own ground truth file to `data/synthetic_csv/ground_truth.csv`\n",
        "- Upload a new blood pressure dataset to `data/synthetic_csv/`\n",
        "\n",
        "Your files will be automatically used by the pipeline — no code modification needed.\n",
        "\n",
        "---\n",
        "\n",
        "### 🗂 Folder Overview\n",
        "\n",
        "| Folder | Purpose |\n",
        "|---------|----------|\n",
        "| `data/synthetic_csv/` | Synthetic or user-provided CSV datasets and ground truth |\n",
        "| `data/Spanish_audio/` | Input Spanish `.wav` question audio files |\n",
        "| `results/llm_outputs/` | LLM-generated question–answer CSV outputs |\n",
        "| `results/evaluation_metrics/` | ASR evaluation metrics (WER, CER, SER) |\n",
        "| `results/tts_audio/` | Generated Spanish audio (TTS) responses |\n",
        "\n",
        "All paths are created automatically under:\n",
        "`/content/drive/MyDrive/HealthTequity-LLM`\n"
      ],
      "metadata": {
        "id": "3o7I6l8g-sXa"
      },
      "id": "3o7I6l8g-sXa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5ec22c8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5ec22c8",
        "outputId": "3d384353-6904-417e-c9a8-36499d638398"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Directory structure verified:\n",
            "📁 Project root:        /content/drive/MyDrive/HealthTequity-LLM\n",
            "📄 Ground truth file:   ✅ Found\n",
            "🎧 Spanish audio files: 9 found\n",
            "📊 Blood pressure CSV:  ✅ Found\n",
            "📊 Results directory:   /content/drive/MyDrive/HealthTequity-LLM/results\n",
            "🧠 LLM outputs:         /content/drive/MyDrive/HealthTequity-LLM/results/llm_outputs\n",
            "🧮 Evaluation metrics:  /content/drive/MyDrive/HealthTequity-LLM/results/evaluation_metrics\n",
            "🔉 TTS audio:           /content/drive/MyDrive/HealthTequity-LLM/results/tts_audio\n"
          ]
        }
      ],
      "source": [
        "# ==========================================================\n",
        "# 📂 STEP 3 — Project Paths and Data Dependencies\n",
        "# ==========================================================\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "# --- Define project root in Google Drive ---\n",
        "PROJECT_ROOT = Path(\"/content/drive/MyDrive/HealthTequity-LLM\")\n",
        "\n",
        "# --- Define subdirectories ---\n",
        "DATA_DIR    = PROJECT_ROOT / \"data\"\n",
        "CSV_DIR     = DATA_DIR / \"synthetic_csv\"\n",
        "AUDIO_DIR   = DATA_DIR / \"Spanish_audio\"\n",
        "RESULTS_DIR = PROJECT_ROOT / \"results\"\n",
        "LLM_OUT     = RESULTS_DIR / \"llm_outputs\"\n",
        "EVAL_DIR    = RESULTS_DIR / \"evaluation_metrics\"\n",
        "TTS_DIR     = RESULTS_DIR / \"tts_audio\"\n",
        "\n",
        "# --- Create folders if missing (safe & repeatable) ---\n",
        "for p in [DATA_DIR, CSV_DIR, AUDIO_DIR, RESULTS_DIR, LLM_OUT, EVAL_DIR, TTS_DIR]:\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# --- Verify generated or uploaded data ---\n",
        "ground_truth = CSV_DIR / \"ground_truth.csv\"\n",
        "audio_files = list(AUDIO_DIR.glob(\"*.wav\"))\n",
        "bp_csv = CSV_DIR / \"synthetic_bp_one_person.csv\"\n",
        "\n",
        "print(\"✅ Directory structure verified:\")\n",
        "print(f\"📁 Project root:        {PROJECT_ROOT}\")\n",
        "print(f\"📄 Ground truth file:   {'✅ Found' if ground_truth.exists() else '⚠️ Missing'}\")\n",
        "print(f\"🎧 Spanish audio files: {len(audio_files)} found\")\n",
        "print(f\"📊 Blood pressure CSV:  {'✅ Found' if bp_csv.exists() else '⚠️ Missing'}\")\n",
        "print(f\"📊 Results directory:   {RESULTS_DIR}\")\n",
        "print(f\"🧠 LLM outputs:         {LLM_OUT}\")\n",
        "print(f\"🧮 Evaluation metrics:  {EVAL_DIR}\")\n",
        "print(f\"🔉 TTS audio:           {TTS_DIR}\")\n",
        "\n",
        "if not ground_truth.exists() or not audio_files:\n",
        "    print(\"\\n⚠️ Note: Run the Audio Generation Notebook or upload your own data to the above folders.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🔑 Step 4 — OpenAI API Key Initialization\n",
        "\n",
        "This step initializes the OpenAI client for all LLM operations (e.g., translations, question answering).\n",
        "\n",
        "**How it works:**\n",
        "- The code first looks for your API key in the Colab environment (`os.environ[\"OPENAI_API_KEY\"]`).\n",
        "- If no key is found, you’ll be securely prompted to paste it (input remains hidden).\n",
        "- Once entered, the key is stored in the current runtime session for later API calls.\n",
        "\n",
        "> 💡 **Security note:**  \n",
        "> Your key is **not saved permanently** — it will reset when the Colab runtime restarts.  \n",
        "> Reviewers can safely run this section with their own OpenAI API keys.\n"
      ],
      "metadata": {
        "id": "dwCedjfe_c-S"
      },
      "id": "dwCedjfe_c-S"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2147149",
      "metadata": {
        "id": "d2147149",
        "outputId": "d15e54a7-c7cf-4838-de05-2a3cc06321e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ OpenAI API key not found in environment.\n",
            "🔐 Paste your OpenAI API key (input hidden): ··········\n",
            "✅ OpenAI client initialized successfully.\n"
          ]
        }
      ],
      "source": [
        "# ==========================================================\n",
        "# 🔑 STEP 4 — OpenAI API Key Initialization\n",
        "# ==========================================================\n",
        "import os\n",
        "from getpass import getpass\n",
        "from openai import OpenAI\n",
        "\n",
        "# --- Check if API key exists; if not, prompt user securely ---\n",
        "if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "    print(\"⚠️ OpenAI API key not found in environment.\")\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"🔐 Paste your OpenAI API key (input hidden): \").strip()\n",
        "else:\n",
        "    print(\"✅ Found existing OpenAI API key in environment.\")\n",
        "\n",
        "# --- Initialize client (raises error if key invalid) ---\n",
        "try:\n",
        "    client = OpenAI()\n",
        "    print(\"✅ OpenAI client initialized successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Failed to initialize OpenAI client: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🗣️ Step 5 — ASR (Whisper) and Spanish → English Translation\n",
        "\n",
        "This section runs the **Automatic Speech Recognition (ASR)** and **translation** stage of the pipeline.  \n",
        "It is used on both sides of the workflow:\n",
        "\n",
        "1. **Input side** – transcribes Spanish question audio and translates it to English for LLM analysis.  \n",
        "2. **Output side** – re-transcribes generated Spanish TTS responses for ASR evaluation.\n",
        "\n",
        "---\n",
        "\n",
        "### ⚙️ How It Works\n",
        "- **Whisper ASR** performs transcription directly from audio.  \n",
        "- **Whisper Translation Mode** (`task=\"translate\"`) produces English text automatically — no API key required.  \n",
        "- Optionally, reviewers can enable **OpenAI GPT translation** for more fluent medical phrasing.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔧 Configuration\n",
        "You can control translation behavior with this flag:\n",
        "```python\n",
        "USE_GPT_TRANSLATION = True  # Set True to use GPT-based translation instead of Whisper\n"
      ],
      "metadata": {
        "id": "Y4vB58i5CTN_"
      },
      "id": "Y4vB58i5CTN_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0128eb68",
      "metadata": {
        "id": "0128eb68"
      },
      "outputs": [],
      "source": [
        "# ==========================================================\n",
        "# 🗣️ STEP 5 — ASR (Whisper) and Spanish → English Translation\n",
        "# ==========================================================\n",
        "import whisper\n",
        "import pandas as pd\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# --- Toggle for optional GPT translation ---\n",
        "USE_GPT_TRANSLATION = True   # Default: Whisper only (free)\n",
        "WHISPER_MODEL_SIZE = \"base\"   # Adjustable model size (\"tiny\", \"small\", \"medium\", \"large\")\n",
        "\n",
        "def transcribe_spanish_audio(model, audio_path: Path):\n",
        "    \"\"\"\n",
        "    Transcribe a single Spanish audio file using Whisper.\n",
        "    Returns Spanish text and detected language.\n",
        "    \"\"\"\n",
        "    result = model.transcribe(str(audio_path), language=\"es\", task=\"transcribe\", verbose=False)\n",
        "    return result[\"text\"].strip(), result.get(\"language\", \"unknown\")\n",
        "\n",
        "def translate_audio_whisper(model, audio_path: Path):\n",
        "    \"\"\"\n",
        "    Use Whisper’s translation mode to directly translate Spanish → English.\n",
        "    \"\"\"\n",
        "    result = model.transcribe(str(audio_path), task=\"translate\", verbose=False)\n",
        "    return result[\"text\"].strip()\n",
        "\n",
        "def translate_spanish_to_english_gpt(spanish_text: str) -> str:\n",
        "    \"\"\"\n",
        "    Optional GPT translation for higher fluency (requires OpenAI API key).\n",
        "    \"\"\"\n",
        "    prompt = (\n",
        "        \"Translate the following Spanish medical transcription into clear, faithful English:\\n\\n\"\n",
        "        + spanish_text\n",
        "    )\n",
        "    result = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        temperature=0,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "    )\n",
        "    return result.choices[0].message.content.strip()\n",
        "\n",
        "def process_and_translate_audio(audio_folder: Path, output_csv: Path,\n",
        "                                model_size: str = WHISPER_MODEL_SIZE) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Run Whisper ASR on all .wav files in a folder,\n",
        "    translate Spanish → English (Whisper or GPT),\n",
        "    and save results to CSV.\n",
        "    \"\"\"\n",
        "    print(f\"🎧 Loading Whisper model: {model_size}\")\n",
        "    model = whisper.load_model(model_size)\n",
        "\n",
        "    audio_files = sorted([f for f in os.listdir(audio_folder) if f.endswith(\".wav\")])\n",
        "    if not audio_files:\n",
        "        print(f\"⚠️ No audio files found in {audio_folder}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    results = []\n",
        "    print(f\"🔍 Processing {len(audio_files)} audio files...\")\n",
        "\n",
        "    for fname in audio_files:\n",
        "        audio_path = audio_folder / fname\n",
        "        # 1️⃣ Spanish transcription\n",
        "        es_text, detected_lang = transcribe_spanish_audio(model, audio_path)\n",
        "        # 2️⃣ Translation (Whisper → English or GPT optional)\n",
        "        if USE_GPT_TRANSLATION:\n",
        "            en_text = translate_spanish_to_english_gpt(es_text)\n",
        "        else:\n",
        "            en_text = translate_audio_whisper(model, audio_path)\n",
        "\n",
        "        results.append({\n",
        "            \"audio_file\": fname,\n",
        "            \"spanish_transcription\": es_text,\n",
        "            \"english_translation\": en_text,\n",
        "            \"language_detected\": detected_lang\n",
        "        })\n",
        "        print(f\"✅ {fname} → processed\")\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    df.to_csv(output_csv, index=False, encoding=\"utf-8-sig\")\n",
        "    print(f\"\\n💾 Results saved to: {output_csv}\")\n",
        "    return df\n",
        "\n",
        "# Example (do not auto-run):\n",
        "# trans_csv = LLM_OUT / \"audio_translations.csv\"\n",
        "# trans_df = process_and_translate_audio(AUDIO_DIR, trans_csv, model_size=\"base\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧮 Step 6 — Unified ASR Evaluation (WER / CER / SER)\n",
        "\n",
        "This section defines a **single reusable evaluation module** for computing ASR performance metrics on any\n",
        "pair of *ground-truth* and *predicted* transcriptions.\n",
        "\n",
        "It is used in two contexts:\n",
        "1. **Input side evaluation** – compares Whisper’s transcribed Spanish questions against the ground-truth CSV.  \n",
        "2. **Output side evaluation** – compares Whisper’s re-transcription of TTS-generated Spanish responses\n",
        "   against the LLM-generated ground-truth Spanish text.\n",
        "\n",
        "---\n",
        "\n",
        "### 📏 Metrics Computed\n",
        "| Metric | Description |\n",
        "|---------|--------------|\n",
        "| **WER (Word Error Rate)** | Fraction of words incorrectly predicted (insertions + deletions + substitutions) / total words |\n",
        "| **CER (Character Error Rate)** | Character-level edit distance normalized by text length |\n",
        "| **SER (Sentence Error Rate)** | Percentage of sentences that are not identical to ground truth |\n",
        "\n",
        "---\n",
        "\n",
        "### ⚙️ How to Use\n",
        "Call the function below with:\n",
        "```python\n",
        "evaluate_asr_whisper(\n",
        "    gt_csv=Path(...),          # CSV with ground_truth_text\n",
        "    audio_folder=Path(...),    # folder of .wav files to evaluate\n",
        "    output_csv=Path(...),      # where to save evaluation results\n",
        "    model_size=\"base\"          # whisper model size (default)\n",
        ")\n"
      ],
      "metadata": {
        "id": "B0hE4n4YDQpV"
      },
      "id": "B0hE4n4YDQpV"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gJMncazPoep-"
      },
      "id": "gJMncazPoep-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24d16763",
      "metadata": {
        "id": "24d16763"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ==========================================================\n",
        "# 🧩 STEP 8 — UNIFIED ASR EVALUATION FUNCTION (FINAL)\n",
        "# ==========================================================\n",
        "import os\n",
        "import pandas as pd\n",
        "import whisper\n",
        "import re, unicodedata, Levenshtein\n",
        "from jiwer import process_words\n",
        "from pathlib import Path\n",
        "\n",
        "def normalize_text(text: str) -> str:\n",
        "    \"\"\"Lowercase, strip accents, and remove punctuation.\"\"\"\n",
        "    text = text.lower()\n",
        "    text = ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', text)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
        "    return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "def evaluate_asr_whisper(\n",
        "    gt_csv: Path,\n",
        "    audio_folder: Path,\n",
        "    output_csv: Path,\n",
        "    model_size: str = \"base\",\n",
        "    gt_text_col: str = \"ground_truth_text\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Evaluate ASR performance using Whisper and compute WER, CER, SER.\n",
        "    Works for both input and output sides of the pipeline.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    gt_csv : Path\n",
        "        CSV file containing at least columns [audio_file, ground_truth_text]\n",
        "    audio_folder : Path\n",
        "        Directory containing .wav audio files to evaluate\n",
        "    output_csv : Path\n",
        "        Output CSV for detailed ASR metrics\n",
        "    model_size : str, optional\n",
        "        Whisper model size to load (default: 'base')\n",
        "        Options: 'tiny', 'base', 'small', 'medium', 'large'\n",
        "    gt_text_col : str, optional\n",
        "        Column name for the reference text (default: 'ground_truth_text')\n",
        "    \"\"\"\n",
        "    if not Path(gt_csv).exists():\n",
        "        raise FileNotFoundError(f\"❌ Ground truth CSV not found at: {gt_csv}\")\n",
        "    if not Path(audio_folder).exists():\n",
        "        raise FileNotFoundError(f\"❌ Audio folder not found at: {audio_folder}\")\n",
        "\n",
        "    df_gt = pd.read_csv(gt_csv)\n",
        "    if \"audio_file\" not in df_gt.columns or gt_text_col not in df_gt.columns:\n",
        "        raise ValueError(f\"❌ CSV must contain ['audio_file', '{gt_text_col}'] columns.\")\n",
        "\n",
        "    print(f\"🎧 Loading Whisper model: {model_size}\")\n",
        "    model = whisper.load_model(model_size)\n",
        "\n",
        "    results = []\n",
        "    print(f\"🔍 Evaluating {len(df_gt)} audio files...\")\n",
        "\n",
        "    for _, row in df_gt.iterrows():\n",
        "        audio_name = row[\"audio_file\"]\n",
        "        gt_text = str(row[gt_text_col]).strip()\n",
        "        audio_path = Path(audio_folder) / audio_name\n",
        "        if not audio_path.exists():\n",
        "            print(f\"⚠️ Missing audio file: {audio_name}\")\n",
        "            continue\n",
        "\n",
        "        # --- Transcribe ---\n",
        "        result = model.transcribe(str(audio_path), language=\"es\", task=\"transcribe\", verbose=False)\n",
        "        hyp_text = result[\"text\"].strip()\n",
        "\n",
        "        # --- Normalize & compute metrics ---\n",
        "        gt_norm, hyp_norm = normalize_text(gt_text), normalize_text(hyp_text)\n",
        "        measures = process_words(gt_norm, hyp_norm)\n",
        "        wer = round(measures.wer, 4)\n",
        "        cer = round(Levenshtein.distance(gt_norm, hyp_norm) / max(len(gt_norm), 1), 4)\n",
        "        ser = 0 if gt_norm == hyp_norm else 1\n",
        "\n",
        "        results.append({\n",
        "            \"audio_file\": audio_name,\n",
        "            \"ground_truth\": gt_text,\n",
        "            \"whisper_transcription\": hyp_text,\n",
        "            \"WER\": wer,\n",
        "            \"CER\": cer,\n",
        "            \"SER\": ser,\n",
        "            \"Substitutions\": measures.substitutions,\n",
        "            \"Deletions\": measures.deletions,\n",
        "            \"Insertions\": measures.insertions\n",
        "        })\n",
        "        print(f\"✅ {audio_name} → WER={wer}, CER={cer}, SER={ser}\")\n",
        "\n",
        "    # --- Save results ---\n",
        "    out_df = pd.DataFrame(results)\n",
        "    out_df.to_csv(output_csv, index=False, encoding=\"utf-8-sig\")\n",
        "    print(f\"\\n💾 ASR metrics saved to: {output_csv}\")\n",
        "    print(f\"📊 Average WER={out_df['WER'].mean():.3f}, CER={out_df['CER'].mean():.3f}, SER={out_df['SER'].mean():.3f}\")\n",
        "    return out_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📊 Step 7 — ASR Metrics Visualization (Input vs Output)\n",
        "\n",
        "This step visualizes the ASR performance of the pipeline for both the **input** and **output** stages.\n",
        "\n",
        "**Purpose:**\n",
        "- The **Input ASR** evaluates how well Whisper transcribes Spanish question audios compared to ground truth.  \n",
        "- The **Output ASR** evaluates Whisper’s accuracy in re-transcribing the TTS-generated Spanish answers.\n",
        "\n",
        "The plot shows average **WER (Word Error Rate)**, **CER (Character Error Rate)**, and **SER (Sentence Error Rate)** side-by-side.\n",
        "\n",
        "**Expected Input Files:**\n",
        "- `results/evaluation_metrics/input_asr_metrics.csv`\n",
        "- `results/evaluation_metrics/output_asr_metrics.csv`\n",
        "\n",
        "**Output:**\n",
        "- A bar chart saved as `results/evaluation_metrics/asr_comparison_chart.png`\n"
      ],
      "metadata": {
        "id": "U7dkMDTZIYQR"
      },
      "id": "U7dkMDTZIYQR"
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# 📊 STEP 7 — ASR Metrics Visualization (Input vs Output)\n",
        "# ==========================================================\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "def load_asr_summary(csv_path: Path):\n",
        "    \"\"\"Load ASR metrics and compute average WER, CER, SER.\"\"\"\n",
        "    if not csv_path.exists():\n",
        "        print(f\"⚠️ Missing file: {csv_path}\")\n",
        "        return {\"WER\": None, \"CER\": None, \"SER\": None}\n",
        "    df = pd.read_csv(csv_path)\n",
        "    return {\n",
        "        \"WER\": df[\"WER\"].mean(),\n",
        "        \"CER\": df[\"CER\"].mean(),\n",
        "        \"SER\": df[\"SER\"].mean()\n",
        "    }\n",
        "\n",
        "def plot_asr_comparison(\n",
        "    input_asr_csv: Path,\n",
        "    output_asr_csv: Path,\n",
        "    output_dir: Path\n",
        "):\n",
        "    \"\"\"\n",
        "    Compare average ASR metrics between input and output sides,\n",
        "    and save a side-by-side bar chart visualization.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_asr_csv : Path\n",
        "        CSV file containing input-side ASR metrics.\n",
        "    output_asr_csv : Path\n",
        "        CSV file containing output-side ASR metrics.\n",
        "    output_dir : Path\n",
        "        Directory where the visualization will be saved.\n",
        "    \"\"\"\n",
        "    # --- Load average metrics ---\n",
        "    input_metrics = load_asr_summary(input_asr_csv)\n",
        "    output_metrics = load_asr_summary(output_asr_csv)\n",
        "\n",
        "    # --- Prepare data for plotting ---\n",
        "    metrics = [\"WER\", \"CER\", \"SER\"]\n",
        "    input_vals = [input_metrics[m] for m in metrics]\n",
        "    output_vals = [output_metrics[m] for m in metrics]\n",
        "\n",
        "    # --- Plot chart ---\n",
        "    fig, ax = plt.subplots(figsize=(7, 5))\n",
        "    x = range(len(metrics))\n",
        "    width = 0.35\n",
        "\n",
        "    ax.bar([i - width/2 for i in x], input_vals, width, label=\"Input ASR\", alpha=0.8)\n",
        "    ax.bar([i + width/2 for i in x], output_vals, width, label=\"Output ASR\", alpha=0.8)\n",
        "\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(metrics)\n",
        "    ax.set_ylabel(\"Error Rate\")\n",
        "    ax.set_ylim(0, 1)\n",
        "    ax.set_title(\"ASR Performance Comparison — Input vs Output\")\n",
        "    ax.legend()\n",
        "    ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # --- Save chart ---\n",
        "    save_path = Path(output_dir) / \"asr_comparison_chart.png\"\n",
        "    plt.savefig(save_path, dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"📊 ASR comparison chart saved to: {save_path}\")\n",
        "    return save_path\n",
        "\n"
      ],
      "metadata": {
        "id": "314K3oxCIaDu"
      },
      "id": "314K3oxCIaDu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧠 Step 8 — LLM Question Answering (Blood Pressure Analysis)\n",
        "\n",
        "This step defines the **LLM-based question-answering function** for the HealthTequity Voice Pipeline.  \n",
        "The model uses the translated English questions from the Spanish audio and answers them using the provided synthetic blood-pressure dataset.\n",
        "\n",
        "---\n",
        "\n",
        "### ⚙️ How It Works\n",
        "- The dataset is passed to the model as a **CSV text block** (context).  \n",
        "- Each question (translated from Spanish → English) is analyzed in context.  \n",
        "- The LLM returns a structured **JSON response** containing:\n",
        "  - `\"answer\"` — a natural-language English explanation.\n",
        "  - `\"computed_fields\"` — optional numeric values or summaries used in reasoning.\n",
        "\n",
        "---\n",
        "\n",
        "### 📚 Behavior Rules\n",
        "- The LLM can only use the dataset for factual answers.  \n",
        "- It can cite external information **only** when describing “normal” blood pressure ranges.  \n",
        "- All outputs follow a conversational, user-friendly tone.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧩 Usage Example\n",
        "```python\n",
        "csv_text = (CSV_DIR / \"synthetic_bp_one_person.csv\").read_text()\n",
        "q = \"What is my average blood pressure this week?\"\n",
        "response = ask_gpt(q, csv_text)\n",
        "print(response[\"answer\"])\n"
      ],
      "metadata": {
        "id": "eu9pvHxqJjH2"
      },
      "id": "eu9pvHxqJjH2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "{\n",
        "  \"answer\": \"Your average systolic pressure this week was 118 mm Hg and your diastolic pressure was 77 mm Hg.\",\n",
        "  \"computed_fields\": {\"systolic_avg\": 118, \"diastolic_avg\": 77}\n",
        "}\n"
      ],
      "metadata": {
        "id": "uvo1FsVgJyk7"
      },
      "id": "uvo1FsVgJyk7"
    },
    {
      "cell_type": "markdown",
      "id": "f4477ecb",
      "metadata": {
        "id": "f4477ecb"
      },
      "source": [
        "\n",
        "## Step 3 – LLM Question Answering <a id=\"qa\"></a>\n",
        "This section queries an LLM with English questions derived from the ASR+translation step and provides answers based on a tabular blood-pressure dataset.\n",
        "\n",
        "**Inputs**: A CSV file with synthetic blood-pressure records.  \n",
        "**Outputs**: English answers and associated computed fields.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85f89584",
      "metadata": {
        "id": "85f89584"
      },
      "outputs": [],
      "source": [
        "\n",
        "#  ==========================================================\n",
        "# 🧠 STEP 8 — LLM Question Answering (Blood Pressure Analysis)\n",
        "# ==========================================================\n",
        "import json\n",
        "\n",
        "def ask_gpt(question_en: str, csv_block: str) -> dict:\n",
        "    \"\"\"\n",
        "    Query the LLM with an English question and the CSV dataset context.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    question_en : str\n",
        "        English question derived from Spanish transcription.\n",
        "    csv_block : str\n",
        "        CSV content as a text block for in-context grounding.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        Dictionary containing:\n",
        "        - \"answer\" : str — model's English response\n",
        "        - \"computed_fields\" : dict — optional numeric details\n",
        "    \"\"\"\n",
        "\n",
        "    system_prompt = \"\"\"\n",
        "You are a careful and detail-oriented data analyst.\n",
        "\n",
        "You are given a synthetic blood pressure dataset in CSV format. It contains readings for one individual over the last 30 consecutive days, with the following columns:\n",
        "\n",
        "- date\n",
        "- age\n",
        "- sex\n",
        "- systolic_mmHg\n",
        "- diastolic_mmHg\n",
        "\n",
        "Use only the data in the CSV to answer all questions, except when normal blood pressure ranges are requested — in those cases, you may use external references but must cite your source.\n",
        "\n",
        "---\n",
        "\n",
        "🧠 Interpretation Guidelines:\n",
        "\n",
        "- \"Today\" refers to the most recent date in the dataset.\n",
        "- \"Yesterday\" means the date before \"today\" in the dataset.\n",
        "- \"Last week\" or \"last month\" refer to 7- or 30-day windows before \"today\".\n",
        "- If a date or range is unavailable, clearly say so.\n",
        "- Use conversational date formats like “October 12” instead of numeric ones.\n",
        "\n",
        "---\n",
        "\n",
        "💬 Answer Style:\n",
        "- Use natural, conversational English.\n",
        "- Address the user as “you”.\n",
        "- Respond clearly and concisely.\n",
        "\n",
        "---\n",
        "\n",
        "✅ Response Format:\n",
        "Always return valid JSON with this structure:\n",
        "\n",
        "{\n",
        "  \"answer\": \"<English answer>\",\n",
        "  \"computed_fields\": { \"numeric values used\" }\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "    user_prompt = (\n",
        "        f\"CSV Dataset:\\n{csv_block}\\n\\n\"\n",
        "        f\"Question:\\n{question_en}\\n\\n\"\n",
        "        \"Please analyze the data and respond strictly in valid JSON format as defined above.\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            temperature=0,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt.strip()},\n",
        "                {\"role\": \"user\", \"content\": user_prompt.strip()},\n",
        "            ],\n",
        "        )\n",
        "        answer_text = response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"❌ OpenAI API error: {e}\")\n",
        "        return {\"answer\": \"Error: failed to retrieve response.\", \"computed_fields\": {}}\n",
        "\n",
        "    # Safely handle JSON parsing\n",
        "    try:\n",
        "        result = json.loads(answer_text)\n",
        "        if not isinstance(result, dict):\n",
        "            raise ValueError(\"Invalid JSON structure.\")\n",
        "    except Exception:\n",
        "        result = {\"answer\": answer_text, \"computed_fields\": {}}\n",
        "\n",
        "    return result\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🔊 Step 9 — Spanish Translation & Text-to-Speech (Optimized)\n",
        "\n",
        "This module converts each English answer from the LLM into natural-sounding **Spanish audio**.  \n",
        "It provides two flexible layers:\n",
        "\n",
        "1. **Translation (English → Spanish)** — by default uses **OpenAI GPT-4o-mini**,  \n",
        "   but automatically falls back to `googletrans` if an API key is not available.\n",
        "2. **Text-to-Speech (Spanish → Audio)** — uses **gTTS + pydub** (free and Colab-friendly).\n",
        "\n",
        "All outputs are saved as `.wav` files in the `results/tts_audio/` directory.\n",
        "\n",
        "---\n",
        "\n",
        "### 📥 Input / 📤 Output\n",
        "\n",
        "| Step | Input | Output |\n",
        "|------|--------|---------|\n",
        "| Translation | English text | Spanish text |\n",
        "| TTS | Spanish text | Spoken Spanish `.wav` file |\n"
      ],
      "metadata": {
        "id": "ma2u3TE7ML5w"
      },
      "id": "ma2u3TE7ML5w"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p4RdoHuOlzBf"
      },
      "id": "p4RdoHuOlzBf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f48ecab5",
      "metadata": {
        "id": "f48ecab5"
      },
      "outputs": [],
      "source": [
        "# ==========================================================\n",
        "# 🔊 STEP 9 — SPANISH TRANSLATION & TEXT-TO-SPEECH (OPTIMIZED)\n",
        "# ==========================================================\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "def translate_to_spanish(text_en: str) -> str:\n",
        "    \"\"\"\n",
        "    Translate English text to Spanish.\n",
        "    Uses OpenAI GPT if available, otherwise falls back to googletrans.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    text_en : str\n",
        "        English text to translate.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        Spanish translation.\n",
        "    \"\"\"\n",
        "    # --- 1️⃣ Try OpenAI translation first ---\n",
        "    try:\n",
        "        if \"client\" in globals():\n",
        "            prompt = (\n",
        "                \"Translate the following English medical answer into clear, \"\n",
        "                \"natural Spanish:\\n\\n\" + text_en\n",
        "            )\n",
        "            resp = client.chat.completions.create(\n",
        "                model=\"gpt-4o-mini\",\n",
        "                temperature=0,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            )\n",
        "            return resp.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ OpenAI translation failed: {e}\")\n",
        "\n",
        "    # --- 2️⃣ Fallback: googletrans (free) ---\n",
        "    try:\n",
        "        from googletrans import Translator\n",
        "        translator = Translator()\n",
        "        return translator.translate(text_en, src=\"en\", dest=\"es\").text\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ googletrans fallback failed: {e}\")\n",
        "        return \"(translation unavailable)\"\n",
        "\n",
        "def text_to_speech_spanish(text_es: str, out_wav_path: Path):\n",
        "    \"\"\"\n",
        "    Generate Spanish TTS audio (free fallback using gTTS + pydub).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    text_es : str\n",
        "        Spanish text to synthesize.\n",
        "    out_wav_path : Path\n",
        "        Destination path for the WAV file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from gtts import gTTS\n",
        "        from pydub import AudioSegment\n",
        "\n",
        "        out_wav_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        tmp_mp3 = out_wav_path.with_suffix(\".mp3\")\n",
        "\n",
        "        # Generate MP3 and convert to WAV\n",
        "        gTTS(text_es, lang=\"es\").save(tmp_mp3)\n",
        "        AudioSegment.from_mp3(tmp_mp3).export(out_wav_path, format=\"wav\")\n",
        "        os.remove(tmp_mp3)\n",
        "\n",
        "        print(f\"✅ Spanish TTS saved → {out_wav_path.name}\")\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"TTS generation failed: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🚀 Step 10 — Full Pipeline Integration\n",
        "\n",
        "This step executes the **complete HealthTequity Voice Pipeline**, combining all previous components into a single, reproducible workflow.\n",
        "\n",
        "The pipeline processes spoken Spanish questions, interprets them through an AI-driven analytics system, and produces accurate Spanish spoken answers grounded in tabular blood-pressure data.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧩 Workflow Overview\n",
        "\n",
        "| Step | Process | Description | Output |\n",
        "|------|----------|--------------|---------|\n",
        "| 1️⃣ | **Input ASR + Translation** | Transcribes each Spanish question and translates it into English. | `audio_translations.csv` |\n",
        "| 2️⃣ | **Input ASR Evaluation** | Compares Whisper transcriptions with ground-truth text to compute WER, CER, and SER. | `input_asr_metrics.csv` |\n",
        "| 3️⃣ | **CSV Grounding** | Loads the synthetic blood-pressure dataset as the context for LLM reasoning. | In-memory CSV string |\n",
        "| 4️⃣ | **LLM Question Answering** | Uses GPT to analyze the dataset and answer each English question. | English answer text |\n",
        "| 5️⃣ | **Spanish Translation + TTS** | Converts each English answer into natural Spanish and generates speech audio. | `results/tts_audio/*.wav` |\n",
        "| 6️⃣ | **Output ASR Evaluation** | Transcribes TTS-generated Spanish audio and compares it to ground-truth Spanish answers. | `output_asr_metrics.csv` |\n",
        "| 7️⃣ | **Visualization** | Displays a side-by-side comparison of WER, CER, and SER for input vs. output ASR. | Matplotlib chart |\n",
        "\n",
        "---\n",
        "\n",
        "### 📁 Input Requirements\n",
        "- **`data/synthetic_csv/synthetic_bp_one_person.csv`** — Blood-pressure dataset  \n",
        "- **`data/Spanish_audio/`** — Spanish question audio files  \n",
        "- **`data/synthetic_csv/ground_truth.csv`** — Ground-truth transcription for ASR evaluation  \n",
        "\n",
        "Reviewers may replace these files with their own data to test other datasets or audio samples.\n",
        "\n",
        "---\n",
        "\n",
        "### 💾 Output Artifacts\n",
        "All generated files are automatically stored in the following directories:\n",
        "- **Transcriptions & LLM results:** `results/llm_outputs/`\n",
        "- **Evaluation metrics:** `results/evaluation_metrics/`\n",
        "- **Spanish speech audio:** `results/tts_audio/`\n",
        "\n",
        "---\n",
        "\n",
        "### ⚙️ Configuration\n",
        "You can select different Whisper models for accuracy/speed trade-offs using the parameter:\n",
        "```python\n",
        "whisper_model_size=\"base\"  # options: \"tiny\", \"base\", \"small\", \"medium\", \"large\"\n"
      ],
      "metadata": {
        "id": "CszJZQ6ZOPbk"
      },
      "id": "CszJZQ6ZOPbk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "852067e6",
      "metadata": {
        "id": "852067e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ead418f-accf-4594-d7b6-e42bc79caa3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "🎙️ STEP 1: Input ASR + Translation (Spanish → English)\n",
            "==============================\n",
            "🎧 Loading Whisper model: base\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Processing 9 audio files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1303/1303 [00:04<00:00, 325.25frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ q10_es.wav → processed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 477/477 [00:03<00:00, 156.88frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ q2_es.wav → processed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 427/427 [00:02<00:00, 166.57frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ q3_es.wav → processed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 573/573 [00:02<00:00, 207.54frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ q4_es.wav → processed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 328/328 [00:03<00:00, 83.21frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ q5_es.wav → processed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1461/1461 [00:04<00:00, 346.44frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ q6_es.wav → processed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 739/739 [00:02<00:00, 269.37frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ q7_es.wav → processed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 516/516 [00:02<00:00, 192.25frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ q8_es.wav → processed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 458/458 [00:02<00:00, 176.23frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ q9_es.wav → processed\n",
            "\n",
            "💾 Results saved to: /content/drive/MyDrive/HealthTequity-LLM/results/llm_outputs/audio_translations.csv\n",
            "\n",
            "==============================\n",
            "📊 STEP 2: Evaluate Input ASR (WER / CER / SER)\n",
            "==============================\n",
            "🎧 Loading Whisper model: base\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Evaluating 10 audio files...\n",
            "⚠️ Missing audio file: q1_es.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 477/477 [00:03<00:00, 134.15frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ q2_es.wav → WER=0.0, CER=0.0, SER=0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 427/427 [00:03<00:00, 141.22frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ q3_es.wav → WER=0.0, CER=0.0, SER=0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 573/573 [00:02<00:00, 203.56frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ q4_es.wav → WER=0.0714, CER=0.0128, SER=1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 328/328 [00:02<00:00, 129.30frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ q5_es.wav → WER=0.0, CER=0.0, SER=0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1461/1461 [00:04<00:00, 300.37frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ q6_es.wav → WER=0.0968, CER=0.0595, SER=1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 739/739 [00:03<00:00, 191.49frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ q7_es.wav → WER=0.0, CER=0.0, SER=0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 516/516 [00:02<00:00, 193.63frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ q8_es.wav → WER=0.0, CER=0.0, SER=0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 458/458 [00:03<00:00, 149.34frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ q9_es.wav → WER=0.0, CER=0.0, SER=0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1303/1303 [00:04<00:00, 287.91frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ q10_es.wav → WER=0.0714, CER=0.0121, SER=1\n",
            "\n",
            "💾 ASR metrics saved to: /content/drive/MyDrive/HealthTequity-LLM/results/evaluation_metrics/input_asr_metrics.csv\n",
            "📊 Average WER=0.027, CER=0.009, SER=0.333\n",
            "\n",
            "==============================\n",
            "📈 STEP 3: LLM Question Answering\n",
            "==============================\n",
            "✅ Spanish TTS saved → answer_1_es.wav\n",
            "✅ Spanish TTS saved → answer_2_es.wav\n",
            "✅ Spanish TTS saved → answer_3_es.wav\n",
            "✅ Spanish TTS saved → answer_4_es.wav\n",
            "✅ Spanish TTS saved → answer_5_es.wav\n",
            "✅ Spanish TTS saved → answer_6_es.wav\n",
            "✅ Spanish TTS saved → answer_7_es.wav\n",
            "✅ Spanish TTS saved → answer_8_es.wav\n",
            "✅ Spanish TTS saved → answer_9_es.wav\n",
            "✅ Saved final results to: /content/drive/MyDrive/HealthTequity-LLM/results/llm_outputs/final_pipeline_results.csv\n",
            "\n",
            "==============================\n",
            "🧠 STEP 4: Evaluate Output ASR (WER / CER / SER)\n",
            "==============================\n",
            "🎧 Loading Whisper model: base\n",
            "🔍 Evaluating 9 audio files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "100%|██████████| 8366/8366 [00:28<00:00, 297.45frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ /content/drive/MyDrive/HealthTequity-LLM/results/tts_audio/answer_1_es.wav → WER=2.1892, CER=0.7607, SER=1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16216/16216 [00:51<00:00, 316.15frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ /content/drive/MyDrive/HealthTequity-LLM/results/tts_audio/answer_2_es.wav → WER=1.9787, CER=0.547, SER=1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6703/6703 [00:17<00:00, 377.60frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ /content/drive/MyDrive/HealthTequity-LLM/results/tts_audio/answer_3_es.wav → WER=0.4416, CER=0.1766, SER=1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3484/3484 [00:11<00:00, 296.51frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ /content/drive/MyDrive/HealthTequity-LLM/results/tts_audio/answer_4_es.wav → WER=1.0889, CER=0.2934, SER=1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2066/2066 [00:17<00:00, 121.52frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ /content/drive/MyDrive/HealthTequity-LLM/results/tts_audio/answer_5_es.wav → WER=0.375, CER=0.4762, SER=1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5090/5090 [00:46<00:00, 109.38frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ /content/drive/MyDrive/HealthTequity-LLM/results/tts_audio/answer_6_es.wav → WER=0.127, CER=0.1835, SER=1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6000/6000 [00:19<00:00, 301.75frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ /content/drive/MyDrive/HealthTequity-LLM/results/tts_audio/answer_7_es.wav → WER=0.2286, CER=0.1885, SER=1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1401/1401 [00:03<00:00, 418.31frames/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ /content/drive/MyDrive/HealthTequity-LLM/results/tts_audio/answer_8_es.wav → WER=0.1176, CER=0.1889, SER=1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5623/5623 [00:18<00:00, 296.32frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ /content/drive/MyDrive/HealthTequity-LLM/results/tts_audio/answer_9_es.wav → WER=1.4032, CER=0.3877, SER=1\n",
            "\n",
            "💾 ASR metrics saved to: /content/drive/MyDrive/HealthTequity-LLM/results/evaluation_metrics/output_asr_metrics.csv\n",
            "📊 Average WER=0.883, CER=0.356, SER=1.000\n",
            "\n",
            "==============================\n",
            "📊 STEP 5: Visualize ASR Comparison\n",
            "==============================\n",
            "📊 ASR comparison chart saved to: /content/drive/MyDrive/HealthTequity-LLM/results/evaluation_metrics/asr_comparison_chart.png\n",
            "\n",
            "✅ Full pipeline completed successfully.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "def run_full_pipeline(csv_path: Path, audio_folder: Path, whisper_model_size: str = \"base\"):\n",
        "    \"\"\"\n",
        "    Execute the full HealthTequity voice analysis pipeline.\n",
        "    Includes:\n",
        "      1️⃣ Input-side ASR & translation (Spanish → English)\n",
        "      2️⃣ ASR evaluation (WER / CER / SER)\n",
        "      3️⃣ LLM-driven Q&A on the blood pressure dataset\n",
        "      4️⃣ Spanish translation + TTS output\n",
        "      5️⃣ Output-side ASR evaluation (WER / CER / SER)\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    csv_path : Path\n",
        "        Path to the synthetic blood-pressure CSV.\n",
        "    audio_folder : Path\n",
        "        Directory containing input Spanish .wav files.\n",
        "    whisper_model_size : str, optional\n",
        "        Whisper model size (default: \"base\").\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        Summary dictionary with key file paths for inspection.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n==============================\")\n",
        "    print(\"🎙️ STEP 1: Input ASR + Translation (Spanish → English)\")\n",
        "    print(\"==============================\")\n",
        "\n",
        "    trans_csv = LLM_OUT / \"audio_translations.csv\"\n",
        "    _ = process_and_translate_audio(audio_folder, trans_csv, model_size=whisper_model_size)\n",
        "\n",
        "    print(\"\\n==============================\")\n",
        "    print(\"📊 STEP 2: Evaluate Input ASR (WER / CER / SER)\")\n",
        "    print(\"==============================\")\n",
        "\n",
        "    gt_csv = CSV_DIR / \"ground_truth.csv\"\n",
        "    asr_csv = EVAL_DIR / \"input_asr_metrics.csv\"\n",
        "    _ = evaluate_asr_whisper(\n",
        "        gt_csv=gt_csv,\n",
        "        audio_folder=audio_folder,\n",
        "        output_csv=asr_csv,\n",
        "        model_size=whisper_model_size,\n",
        "        gt_text_col=\"ground_truth\"\n",
        "    )\n",
        "\n",
        "    print(\"\\n==============================\")\n",
        "    print(\"📈 STEP 3: LLM Question Answering\")\n",
        "    print(\"==============================\")\n",
        "\n",
        "    df_bp = pd.read_csv(csv_path)\n",
        "    csv_block = df_bp.to_csv(index=False)\n",
        "    trans_df = pd.read_csv(trans_csv)\n",
        "\n",
        "    results = []\n",
        "    for i, row in trans_df.iterrows():\n",
        "        q_en = row[\"english_translation\"]\n",
        "        ans = ask_gpt(q_en, csv_block)\n",
        "        ans_en = ans.get(\"answer\", \"\").strip()\n",
        "        ans_es = translate_to_spanish(ans_en)\n",
        "\n",
        "        # Generate Spanish TTS output\n",
        "        out_wav = TTS_DIR / f\"answer_{i + 1}_es.wav\"\n",
        "        text_to_speech_spanish(ans_es, out_wav)\n",
        "\n",
        "        results.append({\n",
        "            \"question_number\": i + 1,\n",
        "            \"audio_file_in\": row[\"audio_file\"],\n",
        "            \"spanish_question\": row[\"spanish_transcription\"],\n",
        "            \"english_question\": q_en,\n",
        "            \"english_answer\": ans_en,\n",
        "            \"spanish_answer\": ans_es,\n",
        "            \"audio_file\": str(out_wav),  # ✅ unified column for ASR evaluation\n",
        "            \"computed_fields\": json.dumps(ans.get(\"computed_fields\", {}))\n",
        "        })\n",
        "\n",
        "    final_csv = LLM_OUT / \"final_pipeline_results.csv\"\n",
        "    pd.DataFrame(results).to_csv(final_csv, index=False)\n",
        "    print(f\"✅ Saved final results to: {final_csv}\")\n",
        "\n",
        "    print(\"\\n==============================\")\n",
        "    print(\"🧠 STEP 4: Evaluate Output ASR (WER / CER / SER)\")\n",
        "    print(\"==============================\")\n",
        "\n",
        "    output_asr_csv = EVAL_DIR / \"output_asr_metrics.csv\"\n",
        "    _ = evaluate_asr_whisper(\n",
        "        gt_csv=final_csv,\n",
        "        audio_folder=TTS_DIR,\n",
        "        output_csv=output_asr_csv,\n",
        "        model_size=whisper_model_size,\n",
        "        gt_text_col=\"spanish_answer\"\n",
        "    )\n",
        "\n",
        "    print(\"\\n==============================\")\n",
        "    print(\"📊 STEP 5: Visualize ASR Comparison\")\n",
        "    print(\"==============================\")\n",
        "\n",
        "    plot_asr_comparison(\n",
        "        input_asr_csv=asr_csv,\n",
        "        output_asr_csv=output_asr_csv,\n",
        "        output_dir=EVAL_DIR\n",
        "    )\n",
        "\n",
        "    print(\"\\n✅ Full pipeline completed successfully.\")\n",
        "    return {\n",
        "        \"transcriptions_csv\": str(trans_csv),\n",
        "        \"input_asr_metrics_csv\": str(asr_csv),\n",
        "        \"final_pipeline_csv\": str(final_csv),\n",
        "        \"output_asr_metrics_csv\": str(output_asr_csv)\n",
        "    }\n",
        "\n",
        "\n",
        "# Example (not auto-run in submission)\n",
        "bp_csv = CSV_DIR / \"synthetic_bp_one_person.csv\"\n",
        "_ = run_full_pipeline(bp_csv, AUDIO_DIR, whisper_model_size=\"base\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}