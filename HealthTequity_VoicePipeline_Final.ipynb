{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü©∫ HealthTequity Voice Processing Pipeline\n",
    "\n",
    "This notebook demonstrates an end-to-end **Voice-to-Insight** pipeline for the HealthTequity case study. It processes Spanish medical speech through transcription, translation, GPT-based analysis, and **automatic ASR evaluation** (WER, CER, SER).\n",
    "\n",
    "---\n",
    "\n",
    "### üìò Companion Notebooks\n",
    "Two supplementary notebooks support data generation for this pipeline:\n",
    "\n",
    "| Notebook | Purpose | Output Folder |\n",
    "|-----------|---------|---------------|\n",
    "| **Synthetic Blood Pressure Generator** | Creates a 30-day synthetic blood pressure dataset | `data/synthetic_csv/` |\n",
    "| **Spanish Audio Generator** | Generates Spanish-language health questions from the dataset | `data/Spanish_audio/` |\n",
    "\n",
    "> Reviewers may also upload their own CSVs and audio recordings. To compute WER/CER/SER, provide **ground-truth transcriptions** for the input audio.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Pipeline Overview\n",
    "\n",
    "| Component | Description | Technology |\n",
    "|-----------|-------------|------------|\n",
    "| **ASR (Speech-to-Text)** | Transcribes Spanish medical speech to text | Whisper |\n",
    "| **Translation** | Translates Spanish text ‚Üí English | GPT-4o-mini |\n",
    "| **LLM Analysis** | Answers questions from the BP CSV | GPT-4o-mini |\n",
    "| **(Optional) TTS** | Synthesizes Spanish answers to audio | Any TTS provider |\n",
    "| **ASR Evaluation** | Computes WER/CER/SER for input & output | Whisper + JiWER |\n",
    "\n",
    "---\n",
    "\n",
    "### üí° Notes for Reviewers\n",
    "- Default Whisper model is **\"base\"** but can be changed.\n",
    "- This notebook auto-creates required folders under the project root.\n",
    "- Results are saved as CSVs and a summary chart in `results/`.\n",
    "\n",
    "---\n",
    "\n",
    "### üîó Resources\n",
    "- **GitHub Repository:** [HealthTequity-LLM](https://github.com/nattaran/HealthTequity-LLM)\n",
    "- **Data Folders:**\n",
    "  - `data/synthetic_csv/` ‚Üí blood pressure datasets\n",
    "  - `data/Spanish_audio/` ‚Üí input audio files\n",
    "  - `results/` ‚Üí outputs, evaluations, and charts\n",
    "\n",
    "> Designed for reproducibility in Google Colab.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ==========================================================\n",
    "# üöÄ Quick Start: run the full pipeline in one call\n",
    "# ==========================================================\n",
    "# Estimated runtime: ~2-6 minutes with Whisper 'base' (depends on audio length)\n",
    "\n",
    "from pipeline.main import run_full_pipeline\n",
    "from pipeline.config import CSV_DIR, AUDIO_DIR\n",
    "\n",
    "csv_path = CSV_DIR / \"synthetic_bp_one_person.csv\"\n",
    "run_full_pipeline(csv_path, AUDIO_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ==========================================================\n",
    "# (Optional) Mount Google Drive in Colab\n",
    "# ==========================================================\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive', force_remount=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ==========================================================\n",
    "# Whisper model configuration (adjustable)\n",
    "# ==========================================================\n",
    "# Options: \"tiny\", \"base\", \"small\", \"medium\", \"large\"\n",
    "WHISPER_MODEL_SIZE = \"base\"  # reviewers can change this\n",
    "\n",
    "# Ensure core folders exist (the pipeline.config already does this at import time)\n",
    "from pipeline.config import BASE_DIR, LLM_OUT, EVAL_DIR, TTS_DIR\n",
    "BASE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "LLM_OUT.mkdir(parents=True, exist_ok=True)\n",
    "EVAL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(\"Project root:\", BASE_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ==========================================================\n",
    "# Imports for evaluation and transcription\n",
    "# ==========================================================\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import whisper\n",
    "from pipeline.asr_module import transcribe_spanish_audio\n",
    "from pipeline.evaluation_module import evaluate_asr_pair\n",
    "from pipeline.config import CSV_DIR, AUDIO_DIR\n",
    "print(\"Imports ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚¨õ Step 5 & 6 ‚Äî Unified ASR Evaluation (Input & Output)\n",
    "\n",
    "This section evaluates ASR quality for:\n",
    "- **Input ASR**: original Spanish audio in `data/Spanish_audio/` vs. ground-truth Spanish text\n",
    "- **Output ASR**: generated Spanish TTS in `results/tts_audio/` vs. Spanish answers from LLM results\n",
    "\n",
    "Both use the same evaluation function and save WER/CER/SER per sample to CSV.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Helper: load ground-truth for INPUT side\n",
    "# Looks for CSV at data/synthetic_csv/ground_truth.csv with columns:\n",
    "#   audio_file, ground_truth_text\n",
    "def load_input_ground_truth(csv_dir: Path = CSV_DIR):\n",
    "    gt_csv = csv_dir / \"ground_truth.csv\"\n",
    "    if not gt_csv.exists():\n",
    "        print(f\"‚ö†Ô∏è Ground-truth CSV not found: {gt_csv}\\nCreate it with columns: audio_file, ground_truth_text\")\n",
    "        return None\n",
    "    df = pd.read_csv(gt_csv)\n",
    "    if not {\"audio_file\", \"ground_truth_text\"}.issubset(df.columns):\n",
    "        raise ValueError(\"ground_truth.csv must include columns: audio_file, ground_truth_text\")\n",
    "    # Return texts in the order of audio files present in AUDIO_DIR\n",
    "    audio_files = sorted([p.name for p in (AUDIO_DIR).glob('*.wav')])\n",
    "    mapping = dict(zip(df[\"audio_file\"], df[\"ground_truth_text\"]))\n",
    "    gt_texts = [mapping.get(name, \"\") for name in audio_files]\n",
    "    return audio_files, gt_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Core evaluation routine reused for input and output\n",
    "def evaluate_folder_against_texts(audio_folder: Path, ground_truth_texts, label: str, out_csv: Path,\n",
    "                                  whisper_model_size: str = WHISPER_MODEL_SIZE):\n",
    "    model = whisper.load_model(whisper_model_size)\n",
    "    audio_paths = sorted(audio_folder.glob('*.wav'))\n",
    "    if not audio_paths:\n",
    "        print(f\"‚ö†Ô∏è No .wav files found in {audio_folder}\")\n",
    "        return None\n",
    "    if ground_truth_texts is None:\n",
    "        print(\"‚ö†Ô∏è No ground-truth texts provided; skipping evaluation.\")\n",
    "        return None\n",
    "    if len(ground_truth_texts) != len(audio_paths):\n",
    "        print(f\"‚ö†Ô∏è Mismatch: {len(ground_truth_texts)} GT vs {len(audio_paths)} audio files.\")\n",
    "    # Transcribe each audio\n",
    "    hyp_texts = []\n",
    "    for p in audio_paths:\n",
    "        text, _ = transcribe_spanish_audio(model, p)\n",
    "        hyp_texts.append(text)\n",
    "    # Evaluate\n",
    "    df = evaluate_asr_pair(ground_truth_texts, hyp_texts, label=label, output_csv=out_csv)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------\n",
    "# Input ASR Evaluation (AUDIO_DIR vs. ground_truth.csv)\n",
    "# ----------------------------------------------------------\n",
    "from pipeline.config import EVAL_DIR\n",
    "input_csv_path = EVAL_DIR / \"input_asr_results.csv\"\n",
    "loaded = load_input_ground_truth()\n",
    "if loaded is not None:\n",
    "    audio_names, gt_input_texts = loaded\n",
    "    _ = evaluate_folder_against_texts(\n",
    "        audio_folder=AUDIO_DIR,\n",
    "        ground_truth_texts=gt_input_texts,\n",
    "        label=\"Input ASR Evaluation\",\n",
    "        out_csv=input_csv_path,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------\n",
    "# Output ASR Evaluation (TTS_DIR vs. Spanish answers from LLM)\n",
    "# ----------------------------------------------------------\n",
    "from pipeline.config import TTS_DIR, LLM_OUT\n",
    "gt_output_texts = None\n",
    "final_results_csv = LLM_OUT / \"final_pipeline_results.csv\"\n",
    "fallback_results_csv = LLM_OUT / \"llm_results.csv\"\n",
    "\n",
    "if final_results_csv.exists():\n",
    "    df_res = pd.read_csv(final_results_csv)\n",
    "    if \"spanish_answer\" in df_res.columns:\n",
    "        gt_output_texts = list(df_res[\"spanish_answer\"].astype(str))\n",
    "elif fallback_results_csv.exists():\n",
    "    df_res = pd.read_csv(fallback_results_csv)\n",
    "    if \"spanish_answer\" in df_res.columns:\n",
    "        gt_output_texts = list(df_res[\"spanish_answer\"].astype(str))\n",
    "\n",
    "output_csv_path = EVAL_DIR / \"output_asr_results.csv\"\n",
    "if gt_output_texts is not None:\n",
    "    _ = evaluate_folder_against_texts(\n",
    "        audio_folder=TTS_DIR,\n",
    "        ground_truth_texts=gt_output_texts,\n",
    "        label=\"Output ASR Evaluation\",\n",
    "        out_csv=output_csv_path,\n",
    "    )\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Could not locate Spanish answers in LLM outputs; skipping Output ASR evaluation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Step 7 ‚Äî Summaries & Visualization\n",
    "\n",
    "We load both CSVs, print a quick summary, and plot a simple bar chart comparing mean WER/CER/SER for Input vs Output.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pipeline.config import EVAL_DIR\n",
    "\n",
    "def load_eval(path: Path):\n",
    "    if not path.exists():\n",
    "        print(f\"‚ö†Ô∏è Missing: {path}\")\n",
    "        return None\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "inp = load_eval(EVAL_DIR / \"input_asr_results.csv\")\n",
    "outp = load_eval(EVAL_DIR / \"output_asr_results.csv\")\n",
    "\n",
    "if inp is not None:\n",
    "    print(\"\\nInput ASR (first 5 rows):\")\n",
    "    display(inp.head())\n",
    "if outp is not None:\n",
    "    print(\"\\nOutput ASR (first 5 rows):\")\n",
    "    display(outp.head())\n",
    "\n",
    "if (inp is not None) and (outp is not None):\n",
    "    inp_mean = inp[[\"WER\",\"CER\",\"SER\"]].mean()\n",
    "    out_mean = outp[[\"WER\",\"CER\",\"SER\"]].mean()\n",
    "    metrics_df = pd.DataFrame({\"Input ASR\": inp_mean, \"Output ASR\": out_mean})\n",
    "    ax = metrics_df.plot(kind=\"bar\", figsize=(8,5))\n",
    "    ax.set_title(\"ASR Performance Comparison (Input vs Output)\")\n",
    "    ax.set_ylabel(\"Error Rate\")\n",
    "    ax.set_xticklabels([\"WER\",\"CER\",\"SER\"], rotation=0)\n",
    "    ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plot_path = EVAL_DIR / \"asr_metrics_plot.png\"\n",
    "    plt.savefig(plot_path, dpi=300)\n",
    "    plt.show()\n",
    "    print(f\"\\nüñºÔ∏è Chart saved to: {plot_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üóÇÔ∏è Step 8 ‚Äî Results Summary\n",
    "\n",
    "Key output files generated by this pipeline (download from the `results/` folders):\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from pipeline.config import LLM_OUT\n",
    "print(\"\\nüìÇ Results Directory Summary:\")\n",
    "print(\"‚îÄ\"*40)\n",
    "print(f\"Input ASR Evaluation CSV:  {EVAL_DIR / 'input_asr_results.csv'}\")\n",
    "print(f\"Output ASR Evaluation CSV: {EVAL_DIR / 'output_asr_results.csv'}\")\n",
    "print(f\"LLM Q&A Results CSV:       {LLM_OUT / 'llm_results.csv'}\")\n",
    "print(f\"ASR Metrics Visualization:  {EVAL_DIR / 'asr_metrics_plot.png'}\")\n",
    "print(\"\\n‚úÖ All results saved successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}